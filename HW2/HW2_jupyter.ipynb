{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81a981ae",
   "metadata": {},
   "source": [
    "# Question 5.3: Confusion Matrix Calculations - Recall, Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fdc639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Per-class Metrics ---\n",
      "Class: Cat\n",
      "  Precision: 0.2500\n",
      "  Recall:    0.2500\n",
      "\n",
      "Class: Dog\n",
      "  Precision: 0.4444\n",
      "  Recall:    0.4444\n",
      "\n",
      "Class: Rabbit\n",
      "  Precision: 0.4000\n",
      "  Recall:    0.4000\n",
      "\n",
      "--- Macro-averaged Metrics ---\n",
      "Macro-averaged Precision: 0.3648\n",
      "Macro-averaged Recall:    0.3648\n",
      "\n",
      "--- Micro-averaged Metrics ---\n",
      "Micro-averaged Precision: 0.3889\n",
      "Micro-averaged Recall:    0.3889\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Confusion Matrix as given in the question\n",
    "# Rows: System (Predicted), Columns: Gold (Actual)\n",
    "# Classes: Cat, Dog, Rabbit\n",
    "confusion_matrix = np.array([\n",
    "    [5, 10, 5],   # Predicted Cat\n",
    "    [15, 20, 10],  # Predicted Dog\n",
    "    [0, 15, 10]    # Predicted Rabbit\n",
    "])\n",
    "\n",
    "classes = ['Cat', 'Dog', 'Rabbit']\n",
    "\n",
    "# --- Per-class Precision and Recall ---\n",
    "print(\"--- Per-class Metrics ---\")\n",
    "per_class_precision = []\n",
    "per_class_recall = []\n",
    "for i, cls in enumerate(classes):\n",
    "    tp = confusion_matrix[i, i]\n",
    "    fp = np.sum(confusion_matrix[i, :]) - tp\n",
    "    fn = np.sum(confusion_matrix[:, i]) - tp\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "    per_class_precision.append(precision)\n",
    "    per_class_recall.append(recall)\n",
    "    \n",
    "    print(f\"Class: {cls}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\\n\")\n",
    "\n",
    "# --- Macro-averaged Precision and Recall ---\n",
    "print(\"--- Macro-averaged Metrics ---\")\n",
    "macro_precision = np.mean(per_class_precision)\n",
    "macro_recall = np.mean(per_class_recall)\n",
    "\n",
    "print(f\"Macro-averaged Precision: {macro_precision:.4f}\")\n",
    "print(f\"Macro-averaged Recall:    {macro_recall:.4f}\\n\")\n",
    "\n",
    "# --- Micro-averaged Precision and Recall ---\n",
    "print(\"--- Micro-averaged Metrics ---\")\n",
    "total_tp = np.trace(confusion_matrix) # Sum of diagonal\n",
    "# For multi-class, total FP and FN are the sum of off-diagonal elements\n",
    "total_fp_fn = np.sum(confusion_matrix) - total_tp\n",
    "\n",
    "micro_precision = total_tp / (total_tp + total_fp_fn)\n",
    "micro_recall = total_tp / (total_tp + total_fp_fn)\n",
    "\n",
    "print(f\"Micro-averaged Precision: {micro_precision:.4f}\")\n",
    "print(f\"Micro-averaged Recall:    {micro_recall:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3db474",
   "metadata": {},
   "source": [
    "### Q5.3 Summary of Results\n",
    "\n",
    "**Per-class Metrics:**\n",
    "- **Cat:** Precision = 0.2500, Recall = 0.2500\n",
    "- **Dog:** Precision = 0.4444, Recall = 0.4444\n",
    "- **Rabbit:** Precision = 0.4000, Recall = 0.4000\n",
    "\n",
    "**Averaged Metrics:**\n",
    "- **Macro-averaged Precision:** 0.3648\n",
    "- **Macro-averaged Recall:** 0.3648\n",
    "- **Micro-averaged Precision:** 0.3889\n",
    "- **Micro-averaged Recall:** 0.3889\n",
    "\n",
    "The micro-averaged precision and recall are equal, which is expected, and they both represent the overall accuracy of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5012915a",
   "metadata": {},
   "source": [
    "# Question 8: Bigram Language Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9887962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /home/nidhinninan/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /home/nidhinninan/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/nidhinninan/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from nltk) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in /home/nidhinninan/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from nltk) (4.66.5)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "# Install the nltk package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa9ab51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Counts ---\n",
      "Unigram Counts: {'<s>': 3, 'I': 2, 'love': 2, 'NLP': 1, '</s>': 3, 'deep': 2, 'learning': 2, 'is': 1, 'fun': 1}\n",
      "\n",
      "Bigram Counts: {('<s>', 'I'): 2, ('I', 'love'): 2, ('love', 'NLP'): 1, ('NLP', '</s>'): 1, ('love', 'deep'): 1, ('deep', 'learning'): 2, ('learning', '</s>'): 1, ('<s>', 'deep'): 1, ('learning', 'is'): 1, ('is', 'fun'): 1, ('fun', '</s>'): 1}\n",
      "\n",
      "--- Bigram Probabilities (MLE) ---\n",
      "P(I          | <s>       ) = 0.6667\n",
      "P(love       | I         ) = 1.0000\n",
      "P(NLP        | love      ) = 0.5000\n",
      "P(</s>       | NLP       ) = 1.0000\n",
      "P(deep       | love      ) = 0.5000\n",
      "P(learning   | deep      ) = 1.0000\n",
      "P(</s>       | learning  ) = 0.5000\n",
      "P(deep       | <s>       ) = 0.3333\n",
      "P(is         | learning  ) = 0.5000\n",
      "P(fun        | is        ) = 1.0000\n",
      "P(</s>       | fun       ) = 1.0000\n",
      "\n",
      "--- Sentence Probabilities ---\n",
      "P('<s> I love NLP </s>') = 0.3333\n",
      "P('<s> I love deep learning </s>') = 0.1667\n",
      "\n",
      "--- Model Preference ---\n",
      "The model prefers '<s> I love NLP </s>'.\n",
      "Reason: Its calculated probability (0.3333) is higher than for the other sentence (0.1667).\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Training corpus\n",
    "corpus_raw = [\n",
    "    \"<s> I love NLP </s>\",\n",
    "    \"<s> I love deep learning </s>\",\n",
    "    \"<s> deep learning is fun </s>\"\n",
    "]\n",
    "\n",
    "# Tokenize the corpus by splitting on spaces\n",
    "tokenized_corpus = [sentence.split() for sentence in corpus_raw]\n",
    "\n",
    "# --- Compute unigram and bigram counts ---\n",
    "\n",
    "# Flatten the tokenized corpus to get a single list of all tokens for unigram counting\n",
    "all_tokens = [token for sentence in tokenized_corpus for token in sentence]\n",
    "unigram_counts = Counter(all_tokens)\n",
    "\n",
    "# Generate bigrams from each sentence in the tokenized corpus\n",
    "all_bigrams = [bigram for sentence in tokenized_corpus for bigram in ngrams(sentence, 2)]\n",
    "bigram_counts = Counter(all_bigrams)\n",
    "\n",
    "print(\"--- Counts ---\")\n",
    "print(\"Unigram Counts:\", dict(unigram_counts))\n",
    "print(\"\\nBigram Counts:\", dict(bigram_counts))\n",
    "\n",
    "\n",
    "# --- Estimate bigram probabilities using Maximum Likelihood Estimation (MLE) ---\n",
    "bigram_probabilities = defaultdict(float)\n",
    "for bigram, count in bigram_counts.items():\n",
    "    prefix = bigram[0]\n",
    "    prefix_count = unigram_counts[prefix]\n",
    "    if prefix_count > 0:\n",
    "        bigram_probabilities[bigram] = count / prefix_count\n",
    "\n",
    "print(\"\\n--- Bigram Probabilities (MLE) ---\")\n",
    "for bigram, prob in bigram_probabilities.items():\n",
    "    print(f\"P({bigram[1]:<10} | {bigram[0]:<10}) = {prob:.4f}\")\n",
    "\n",
    "\n",
    "# --- Implement a function to calculate sentence probability ---\n",
    "def calculate_sentence_probability(sentence, bigram_probs):\n",
    "    \"\"\"Calculates the probability of a sentence using a bigram model.\"\"\"\n",
    "    tokens = sentence.split()\n",
    "    sentence_bigrams = ngrams(tokens, 2)\n",
    "    probability = 1.0\n",
    "    for bigram in sentence_bigrams:\n",
    "        # If a bigram was not seen in training, its probability is 0\n",
    "        probability *= bigram_probs.get(bigram, 0)\n",
    "    return probability\n",
    "\n",
    "# --- Test on given sentences ---\n",
    "sentence1 = \"<s> I love NLP </s>\"\n",
    "sentence2 = \"<s> I love deep learning </s>\"\n",
    "\n",
    "prob1 = calculate_sentence_probability(sentence1, bigram_probabilities)\n",
    "prob2 = calculate_sentence_probability(sentence2, bigram_probabilities)\n",
    "\n",
    "print(\"\\n--- Sentence Probabilities ---\")\n",
    "print(f\"P('{sentence1}') = {prob1:.4f}\")\n",
    "print(f\"P('{sentence2}') = {prob2:.4f}\")\n",
    "\n",
    "# --- Print which sentence the model prefers and why ---\n",
    "print(\"\\n--- Model Preference ---\")\n",
    "if prob1 > prob2:\n",
    "    print(f\"The model prefers '{sentence1}'.\")\n",
    "    print(f\"Reason: Its calculated probability ({prob1:.4f}) is higher than for the other sentence ({prob2:.4f}).\")\n",
    "elif prob2 > prob1:\n",
    "    print(f\"The model prefers '{sentence2}'.\")\n",
    "    print(f\"Reason: Its calculated probability ({prob2:.4f}) is higher than for the other sentence ({prob1:.4f}).\")\n",
    "else:\n",
    "    print(\"The model has no preference; both sentences have the same probability.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75be9653",
   "metadata": {},
   "source": [
    "### Q8 Summary of Results\n",
    "\n",
    "The bigram language model was trained on the provided corpus. The probabilities for the two test sentences were calculated as follows:\n",
    "\n",
    "- **P('&lt;s&gt; I love NLP &lt;/s&gt;')**: 0.3333\n",
    "- **P('&lt;s&gt; I love deep learning &lt;/s&gt;')**: 0.1667\n",
    "\n",
    "Based on these probabilities, the model **prefers the sentence ('&lt;s&gt; I love NLP &lt;/s&gt;')**.\n",
    "\n",
    "The reason for this preference is that the bigram `('love', 'NLP')` has a higher conditional probability within this specific training corpus than the bigram `('love', 'deep')`. Both sentences share the common prefix `<s> I love`, but the model assigns a higher likelihood to the sequence continuing with `NLP` than with `deep learning` based on the word co-occurrences it learned from the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7c30bb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
