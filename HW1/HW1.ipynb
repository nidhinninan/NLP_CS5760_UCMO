{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4bacff7",
   "metadata": {},
   "source": [
    "# Homework 1 - Natural Language Processing\n",
    "\n",
    "**Student Name:** Nidhin Ninan \n",
    "\n",
    "**Student UIN:** 700772413 \n",
    "\n",
    "**Date:** Sept 15, 2025 \n",
    "\n",
    "**Course:** CS 5720, Natural Language Processing\n",
    "\n",
    "This notebook contains solutions to Homework 1 covering:\n",
    "1. Regular Expressions\n",
    "2. Tokenization \n",
    "3. Byte Pair Encoding (BPE)\n",
    "4. Edit Distance\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc3acd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages imported successfully!\n",
      "Ready to solve Homework 1 problems.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages for the homework\n",
    "import re\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple, Set\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For visualization (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"All packages imported successfully!\")\n",
    "print(\"Ready to solve Homework 1 problems.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f55b01",
   "metadata": {},
   "source": [
    "# Question 1: Regular Expressions\n",
    "\n",
    "**Task:** Create regex patterns for various text matching requirements.\n",
    "\n",
    "This section contains regex patterns for:\n",
    "- U.S. ZIP codes (whole tokens only)\n",
    "- Words that do NOT start with a capital letter\n",
    "- Numbers with various formats (sign, thousands commas, decimal, scientific)\n",
    "- Spelling variants of \"email\" (case insensitive)\n",
    "- Interjections like \"go\", \"goo\", \"gooo\" with optional punctuation\n",
    "- Lines ending with \"?\" followed by closing quotes/brackets/spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f92111e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1.1 - U.S. ZIP codes pattern:\n",
      "Pattern: \\b\\d{5}(?:[-\\s]\\d{4})?\\b\n",
      "\n",
      "Test results:\n",
      "'12345' -> ['12345']\n",
      "'12345-6789' -> ['12345-6789']\n",
      "'12345 6789' -> ['12345 6789']\n"
     ]
    }
   ],
   "source": [
    "# Q1.1: U.S. ZIP codes (whole tokens only)\n",
    "# Pattern matches 5 digits or 5 digits + hyphen/space + 4 digits\n",
    "zip_pattern = r'\\b\\d{5}(?:[-\\s]\\d{4})?\\b'\n",
    "# zip_pattern = r'\\b\\d{5}([- ]\\d{4})?\\b'  #does work when finding the pattern but will not work when testing the test cases\n",
    "                                        #using re.findall as it only return the last part of the pattern\n",
    "\n",
    "print(\"Q1.1 - U.S. ZIP codes pattern:\")\n",
    "print(f\"Pattern: {zip_pattern}\")\n",
    "\n",
    "# Test cases\n",
    "test_zip_strings = [\n",
    "    \"12345\",           # Valid: 5 digits\n",
    "    \"12345-6789\",      # Valid: 5+4 format with hyphen\n",
    "    \"12345 6789\",      # Valid: 5+4 format with space\n",
    "]\n",
    "\n",
    "print(\"\\nTest results:\")\n",
    "for test_str in test_zip_strings:\n",
    "    matches = re.findall(zip_pattern, test_str)\n",
    "    print(f\"'{test_str}' -> {matches}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29a75c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1.2 - Words NOT starting with capital letter:\n",
      "ASCII pattern: \\b(?![A-Z])[a-z][a-z\\'\\-]*\\b\n",
      "\n",
      "Test text: hello World don't state-of-the-art iPhone e-mail UNESCO\n",
      "Words NOT starting with capital: ['hello', \"don't\", 'state-of-the-art', 'e-mail']\n"
     ]
    }
   ],
   "source": [
    "# Q1.2: Words that do NOT start with a capital letter\n",
    "\n",
    "# ASCII/simple English fallback\n",
    "ascii_pattern = r'\\b(?![A-Z])[a-z][a-z\\'\\-]*\\b'\n",
    "\n",
    "print(\"Q1.2 - Words NOT starting with capital letter:\")\n",
    "print(f\"ASCII pattern: {ascii_pattern}\")\n",
    "\n",
    "# Test cases (using ASCII pattern for compatibility)\n",
    "test_words = [\"hello\", \"World\", \"don't\", \"state-of-the-art\", \"iPhone\", \"e-mail\", \"UNESCO\"]\n",
    "test_text = \" \".join(test_words)\n",
    "\n",
    "print(f\"\\nTest text: {test_text}\")\n",
    "matches = re.findall(ascii_pattern, test_text)\n",
    "print(f\"Words NOT starting with capital: {matches}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e19fd325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1.3 - Number pattern:\n",
      "Pattern: (?<!\\S)[+-]?(?:\\d{1,3}(?:,\\d{3})*|\\d+)(?:\\.\\d+)?(?:[eE][+-]?\\d+)?(?!\\S)\n",
      "\n",
      "Test results:\n",
      "'42' -> ['42']\n",
      "'-12,345.67' -> ['-12,345.67']\n",
      "'+3.2e-4' -> ['+3.2e-4']\n",
      "'1,000' -> ['1,000']\n",
      "'3.14159' -> ['3.14159']\n",
      "'6.022e23' -> ['6.022e23']\n",
      "'-2.5E-10' -> ['-2.5E-10']\n",
      "'abc123' -> []\n",
      "'123abc' -> []\n",
      "\n",
      "Explanation:\n",
      "- (?<!\\S): not preceded by non-whitespace (token boundary)\n",
      "- [+-]?: optional sign\n",
      "- (?:\\d{{1,3}}(?:,\\d{{3}})*|\\d+): thousands format OR regular digits\n",
      "\n",
      "\t- \\d{1,3}: This part matches the beginning of the number. It requires the number to start with one, two, or three digits before the first comma. This handles 1, 12, and 123.\n",
      "\t- (?:,\\d{3}): This is a non-capturing group that matches a \"chunk\" of a comma followed by exactly three digits (like ,000 or ,456).\n",
      "\t\t*This is the most important part for your question. It's a quantifier that applies to the entire group before it (?:,\\d{3}). It means match the preceding group zero or more times.\n",
      "\n",
      "- (?:\\.\\d+)?: optional decimal part\n",
      "- (?:[eE][+-]?\\d+)?: optional scientific notation\n",
      "- (?!\\S): not followed by non-whitespace (token boundary)\n"
     ]
    }
   ],
   "source": [
    "# Q1.3: Numbers (sign, thousands commas, decimal, scientific notation)\n",
    "number_pattern = r'(?<!\\S)[+-]?(?:\\d{1,3}(?:,\\d{3})*|\\d+)(?:\\.\\d+)?(?:[eE][+-]?\\d+)?(?!\\S)'\n",
    "\n",
    "print(\"Q1.3 - Number pattern:\")\n",
    "print(f\"Pattern: {number_pattern}\")\n",
    "\n",
    "# Test cases\n",
    "test_numbers = [\n",
    "    \"42\",\n",
    "    \"-12,345.67\",\n",
    "    \"+3.2e-4\",\n",
    "    \"1,000\",\n",
    "    \"3.14159\",\n",
    "    \"6.022e23\",\n",
    "    \"-2.5E-10\",\n",
    "    \"abc123\",  # Should not match (embedded)\n",
    "    \"123abc\"   # Should not match (embedded)\n",
    "]\n",
    "\n",
    "print(\"\\nTest results:\")\n",
    "for test_num in test_numbers:\n",
    "    matches = re.findall(number_pattern, test_num)\n",
    "    print(f\"'{test_num}' -> {matches}\")\n",
    "\n",
    "print(\"\\nExplanation:\")\n",
    "print(\"- (?<!\\\\S): not preceded by non-whitespace (token boundary)\")\n",
    "print(\"- [+-]?: optional sign\")\n",
    "print(\"- (?:\\\\d{{1,3}}(?:,\\\\d{{3}})*|\\\\d+): thousands format OR regular digits\")\n",
    "print(\"\\n\\t- \\\\d{1,3}: This part matches the beginning of the number. It requires the number to start with one, two, or three digits before the first comma. This handles 1, 12, and 123.\")\n",
    "print(\"\\t- (?:,\\\\d{3}): This is a non-capturing group that matches a \\\"chunk\\\" of a comma followed by exactly three digits (like ,000 or ,456).\")\n",
    "print(\"\\t\\t*This is the most important part for your question. It's a quantifier that applies to the entire group before it (?:,\\\\d{3}). It means match the preceding group zero or more times.\")\n",
    "print(\"\\n- (?:\\\\.\\\\d+)?: optional decimal part\")\n",
    "print(\"- (?:[eE][+-]?\\\\d+)?: optional scientific notation\")\n",
    "print(\"- (?!\\\\S): not followed by non-whitespace (token boundary)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a80b0d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1.4 - Email spelling variants pattern:\n",
      "Pattern: \\b[eE](?:[-– ])?[mM][aA][iI][lL]\\b\n",
      "- Can also usewith re.IGNORECASE flag for case insensitivity\n",
      "\n",
      "Test results:\n",
      "'email' -> ['email']\n",
      "'e-mail' -> ['e-mail']\n",
      "'e mail' -> ['e mail']\n",
      "'e–mail' -> ['e–mail']\n",
      "'Email' -> ['Email']\n",
      "'E-MAIL' -> ['E-MAIL']\n",
      "'emails' -> []\n",
      "'mail' -> []\n"
     ]
    }
   ],
   "source": [
    "# Q1.4: Spelling variants of \"email\" (case insensitive)\n",
    "email_pattern = r'\\b[eE](?:[-– ])?[mM][aA][iI][lL]\\b'\n",
    "\n",
    "print(\"Q1.4 - Email spelling variants pattern:\")\n",
    "print(f\"Pattern: {email_pattern}\")\n",
    "print(\"- Can also use with re.IGNORECASE flag for case insensitivity\")\n",
    "\n",
    "# Test cases\n",
    "test_email_variants = [\n",
    "    \"email\",\n",
    "    \"e-mail\", \n",
    "    \"e mail\",\n",
    "    \"e–mail\",  # en-dash\n",
    "    \"Email\",   # Capital E\n",
    "    \"E-MAIL\",  # All caps\n",
    "    \"emails\",  # Should not match (plural)\n",
    "    \"mail\"   # Should not match\n",
    "]\n",
    "\n",
    "print(\"\\nTest results:\")\n",
    "for variant in test_email_variants:\n",
    "    matches = re.findall(email_pattern, variant) # ,Can also use re.IGNORECASE, (...,re.IGNORECASE) )\n",
    "    print(f\"'{variant}' -> {matches}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8ca68a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1.5 - 'Go' interjection pattern:\n",
      "Pattern: (?<!\\S)go+[!.,?]?(?!\\S)\n",
      "\n",
      "Test results:\n",
      "'go' -> ['go']\n",
      "'goo!' -> ['goo!']\n",
      "'gooo?' -> ['gooo?']\n",
      "'goooo.' -> ['goooo.']\n",
      "'go,' -> ['go,']\n",
      "'going' -> []\n",
      "'ago' -> []\n",
      "'Let's go!' -> ['go!']\n",
      "'Goooo!' -> ['Goooo!']\n"
     ]
    }
   ],
   "source": [
    "# Q1.5: Interjection \"go\", \"goo\", \"gooo\", etc. with optional punctuation\n",
    "go_pattern = r'(?<!\\S)go+[!.,?]?(?!\\S)'\n",
    "\n",
    "print(\"Q1.5 - 'Go' interjection pattern:\")\n",
    "print(f\"Pattern: {go_pattern}\")\n",
    "\n",
    "# Test cases\n",
    "test_go_variants = [\n",
    "    \"go\",\n",
    "    \"goo!\",\n",
    "    \"gooo?\",\n",
    "    \"goooo.\",\n",
    "    \"go,\",\n",
    "    \"going\",    # Should not match\n",
    "    \"ago\",      # Should not match  \n",
    "    \"Let's go!\",\n",
    "    \"Goooo!\"\n",
    "]\n",
    "\n",
    "print(\"\\nTest results:\")\n",
    "for variant in test_go_variants:\n",
    "    matches = re.findall(go_pattern, variant, re.IGNORECASE)\n",
    "    print(f\"'{variant}' -> {matches}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a09e5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1.6 - Lines ending with '?' pattern:\n",
      "Pattern: \\?[ \\t\\\"\\')\\]\\}\\\"\\'»]*$\n",
      "- Can use with re.MULTILINE flag\n",
      "\n",
      "Test results:\n",
      "Lines that match the pattern:\n",
      "Line 1: 'Are you coming?' ✓\n",
      "Line 2: 'Are you coming?\"' ✓\n",
      "Line 3: 'Are you coming?'' ✓\n",
      "Line 4: 'Are you coming?)' ✓\n",
      "Line 5: 'Are you coming?]' ✓\n",
      "Line 6: 'Are you coming? ' ✓\n",
      "Line 7: 'Are you coming?\t' ✓\n",
      "Line 8: 'What about this? No.' ✗\n",
      "Line 9: 'How are you? Fine.' ✗\n"
     ]
    }
   ],
   "source": [
    "# Q1.6: Lines ending with \"?\" followed by closing quotes/brackets and spaces\n",
    "line_end_pattern = r'\\?[ \\t\\\"\\')\\]\\}\\\"\\'»]*$'\n",
    "\n",
    "print(\"Q1.6 - Lines ending with '?' pattern:\")\n",
    "print(f\"Pattern: {line_end_pattern}\")\n",
    "print(\"- Can use with re.MULTILINE flag\")\n",
    "\n",
    "# Test cases (multiline string)\n",
    "test_lines = \"\"\"Are you coming?\n",
    "Are you coming?\"\n",
    "Are you coming?'\n",
    "Are you coming?)\n",
    "Are you coming?]\n",
    "Are you coming? \n",
    "Are you coming?\t\n",
    "What about this? No.\n",
    "How are you? Fine.\"\"\"\n",
    "\n",
    "print(\"\\nTest results:\")\n",
    "print(\"Lines that match the pattern:\")\n",
    "for i, line in enumerate(test_lines.split('\\n'), 1):\n",
    "    if re.search(line_end_pattern, line):\n",
    "        print(f\"Line {i}: '{line}' ✓\")\n",
    "    else:\n",
    "        print(f\"Line {i}: '{line}' ✗\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acd1b7e",
   "metadata": {},
   "source": [
    "# Question 2: Tokenization\n",
    "\n",
    "**Task:** Implement and compare different tokenization methods.\n",
    "\n",
    "This section covers:\n",
    "1. **Naïve tokenization** (space-based splitting)\n",
    "2. **Manual tokenization** (hand-crafted rules)\n",
    "3. **Tool-based tokenization** (regex-based tokenizer)\n",
    "4. **Multiword Expression (MWE) identification**\n",
    "5. **Comparison and reflection**\n",
    "\n",
    "**Test paragraph:**\n",
    "> \"Yesterday I couldn't finish the report — it was a state-of-the-art model, but the dataset didn't cooperate. I emailed Dr. O'Neil at 3:00 p.m., and she'll review it tomorrow. The newer models outperform older ones; however, we're still testing and refining.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e9d449b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test paragraph:\n",
      "\"ഇന്നലെ എനിക്ക് റിപ്പോർട്ട് പൂർത്തിയാക്കാൻ കഴിഞ്ഞില്ല — അത് അത്യാധുനിക മോഡൽ ആയിരുന്നു, പക്ഷേ ഡാറ്റാസെറ്റ് സഹകരിച്ചില്ല. ഞാൻ ഡോ. കെ.എം. നായരിന് 3:00 പി.എം.-ന് ഇമെയിൽ അയച്ചു, അവർ നാളെ അത് പരിശോധിക്കും. പുതിയ മോഡലുകൾ പഴയവയേക്കാൾ മികച്ചതാണ്; എന്നിരുന്നാലും, ഞങ്ങൾ ഇപ്പോഴും പരീക്ഷിച്ചുകൊണ്ടിരിക്കുകയാണ്.\"\n",
      "English Translation\n",
      "\"Yesterday I could not complete the report — it was a state-of-the-art model, but the dataset did not cooperate. I emailed Dr. K.M. Nair at 3:00 PM, they will check it tomorrow. The new models are better than the old ones; however, we are still testing.\"\n",
      "\n",
      "Length: 297 characters\n"
     ]
    }
   ],
   "source": [
    "# Define the test paragraph for tokenization (Malayalam)\n",
    "paragraph = \"\"\"ഇന്നലെ എനിക്ക് റിപ്പോർട്ട് പൂർത്തിയാക്കാൻ കഴിഞ്ഞില്ല — അത് അത്യാധുനിക മോഡൽ ആയിരുന്നു, പക്ഷേ ഡാറ്റാസെറ്റ് സഹകരിച്ചില്ല. ഞാൻ ഡോ. കെ.എം. നായരിന് 3:00 പി.എം.-ന് ഇമെയിൽ അയച്ചു, അവർ നാളെ അത് പരിശോധിക്കും. പുതിയ മോഡലുകൾ പഴയവയേക്കാൾ മികച്ചതാണ്; എന്നിരുന്നാലും, ഞങ്ങൾ ഇപ്പോഴും പരീക്ഷിച്ചുകൊണ്ടിരിക്കുകയാണ്.\"\"\"\n",
    "English_Translation = \"\"\"Yesterday I could not complete the report — it was a state-of-the-art model, but the dataset did not cooperate. I emailed Dr. K.M. Nair at 3:00 PM, they will check it tomorrow. The new models are better than the old ones; however, we are still testing.\"\"\"\n",
    "\n",
    "print(\"Test paragraph:\")\n",
    "print(f'\"{paragraph}\"')\n",
    "\n",
    "print(\"English Translation\")\n",
    "print(f'\"{English_Translation}\"')\n",
    "print(f\"\\nLength: {len(paragraph)} characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a64c3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2.1 - Naïve Tokenization (space-based):\n",
      "Method: text.split()\n",
      "Number of tokens: 33\n",
      "First 10 tokens: ['ഇന്നലെ', 'എനിക്ക്', 'റിപ്പോർട്ട്', 'പൂർത്തിയാക്കാൻ', 'കഴിഞ്ഞില്ല', '—', 'അത്', 'അത്യാധുനിക', 'മോഡൽ', 'ആയിരുന്നു,']\n",
      "Last 10 tokens: ['അത്', 'പരിശോധിക്കും.', 'പുതിയ', 'മോഡലുകൾ', 'പഴയവയേക്കാൾ', 'മികച്ചതാണ്;', 'എന്നിരുന്നാലും,', 'ഞങ്ങൾ', 'ഇപ്പോഴും', 'പരീക്ഷിച്ചുകൊണ്ടിരിക്കുകയാണ്.']\n",
      "\n",
      "Characteristics:\n",
      "- Punctuation attached to neighboring words\n",
      "- Contractions kept intact\n",
      "- Simple and fast but crude\n"
     ]
    }
   ],
   "source": [
    "# Q2.1: Naïve tokenization (space-based splitting)\n",
    "def naive_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Simple space-based tokenization\"\"\"\n",
    "    return text.split()\n",
    "\n",
    "naive_tokens = naive_tokenize(paragraph)\n",
    "\n",
    "print(\"Q2.1 - Naïve Tokenization (space-based):\")\n",
    "print(f\"Method: text.split()\")\n",
    "print(f\"Number of tokens: {len(naive_tokens)}\")\n",
    "print(\"First 10 tokens:\", naive_tokens[:10])\n",
    "print(\"Last 10 tokens:\", naive_tokens[-10:])\n",
    "print(\"\\nCharacteristics:\")\n",
    "print(\"- Punctuation attached to neighboring words\")\n",
    "print(\"- Contractions kept intact\")\n",
    "print(\"- Simple and fast but crude\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61d749a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2.2 - Manual Tokenization:\n",
      "Number of tokens: 42\n",
      "First 15 tokens: ['ഇന്നലെ', 'എനിക്ക്', 'റിപ്പോർട്ട്', 'പൂർത്തിയാക്കാൻ', 'കഴിഞ്ഞില്ല', '—', 'അത്', 'അത്യാധുനിക', 'മോഡൽ', 'ആയിരുന്നു', ',', 'പക്ഷേ', 'ഡാറ്റാസെറ്റ്', 'സഹകരിച്ചില്ല', '.']\n",
      "Last 15 tokens: ['നാളെ', 'അത്', 'പരിശോധിക്കും', '.', 'പുതിയ', 'മോഡലുകൾ', 'പഴയവയേക്കാൾ', 'മികച്ചതാണ്', ';', 'എന്നിരുന്നാലും', ',', 'ഞങ്ങൾ', 'ഇപ്പോഴും', 'പരീക്ഷിച്ചുകൊണ്ടിരിക്കുകയാണ്', '.']\n",
      "\n",
      "Key features:\n",
      " NOTE: these rules are implemented as the HW asked use to implement the rules BUT malayalam language doesn't use ALL these rule\n",
      "\n",
      "- Separates punctuation into own tokens\n",
      "- Splits clitics (couldn't -> could + n't)\n",
      "- Keeps internal apostrophes/hyphens (state-of-the-art)\n"
     ]
    }
   ],
   "source": [
    "# Q2.2: Manual tokenization (hand-crafted rules)\n",
    "def manual_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Manual tokenization with specific rules:\n",
    "    1. Separate general punctuation into own tokens\n",
    "    2. Keep internal apostrophes/hyphens in words  \n",
    "    3. Split clitics (could + n't, she + 'll, we + 're)\n",
    "    NOTE: these rules are implemented as the HW asked use to implement the rules BUT malayalam language doesn't use these rules\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start with naive split\n",
    "    tokens = text.split()\n",
    "    result = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        # Handle punctuation at the end\n",
    "        while token and token[-1] in '.,;:!?\"—':\n",
    "            if len(token) == 1:\n",
    "                result.append(token)\n",
    "                token = \"\"\n",
    "                break\n",
    "            else:\n",
    "                # Remove punctuation and add it separately\n",
    "                result.append(token[:-1])\n",
    "                result.append(token[-1])\n",
    "                token = \"\"  # Set to empty so we don't process it again\n",
    "                break  # Exit the while loop\n",
    "        \n",
    "        if not token:  # If token is empty, skip the rest\n",
    "            continue\n",
    "            \n",
    "        # Handle specific contractions/clitics (only if token still has content)\n",
    "        if token.endswith(\"n't\"):\n",
    "            result.extend([token[:-3], \"n't\"])\n",
    "        elif token.endswith(\"'ll\"):\n",
    "            result.extend([token[:-3], \"'ll\"])\n",
    "        elif token.endswith(\"'re\"):\n",
    "            result.extend([token[:-3], \"'re\"])\n",
    "        elif token.endswith(\"'ve\"):\n",
    "            result.extend([token[:-3], \"'ve\"])\n",
    "        elif token.endswith(\"'d\"):\n",
    "            result.extend([token[:-2], \"'d\"])\n",
    "        else:\n",
    "            result.append(token)\n",
    "    \n",
    "    return result\n",
    "\n",
    "manual_tokens = manual_tokenize(paragraph)\n",
    "\n",
    "print(\"Q2.2 - Manual Tokenization:\")\n",
    "print(f\"Number of tokens: {len(manual_tokens)}\")\n",
    "print(\"First 15 tokens:\", manual_tokens[:15])\n",
    "print(\"Last 15 tokens:\", manual_tokens[-15:])\n",
    "print(\"\\nKey features:\")\n",
    "print(\" NOTE: these rules are implemented as the HW asked use to implement the rules BUT malayalam language doesn't use ALL these rule\")\n",
    "print(\"\\n- Separates punctuation into own tokens\")\n",
    "print(\"- Splits clitics (couldn't -> could + n't)\")\n",
    "print(\"- Keeps internal apostrophes/hyphens (state-of-the-art)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423b4d7d",
   "metadata": {},
   "source": [
    "## NOTE: \n",
    "\n",
    "**The following few cells are included in the notebook as they were required to experiment and install the necessary packages and dependencies for doing tokenisation in the jlab_server environment (Python 3.12.5) that this Jupyter notebook was developed in. RUN THEM ONLY IF NEEDED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351a3985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iNLTK is already installed\n",
      "Error downloading Malayalam model: cannot import name 'Iterable' from 'collections' (/home/nidhinninan/.config/jupyterlab-desktop/jlab_server/lib/python3.12/collections/__init__.py)\n",
      "You may need to run this cell again or check your internet connection.\n",
      "iNLTK setup complete!\n"
     ]
    }
   ],
   "source": [
    "# # Setup for iNLTK Library (run this cell first)\n",
    "# import sys\n",
    "# import os\n",
    "# import subprocess\n",
    "\n",
    "# # Install iNLTK if not already installed\n",
    "# try:\n",
    "#     import inltk\n",
    "#     print(\"iNLTK is already installed\")\n",
    "# except ImportError:\n",
    "#     print(\"Installing iNLTK...\")\n",
    "#     subprocess.check_call([\n",
    "#         sys.executable, \"-m\", \"pip\", \"install\", \"inltk\",\n",
    "#         \"--timeout\", \"1800\",  # 30 minutes timeout\n",
    "#         \"--retries\", \"3\",     # Retry 3 times if failed\n",
    "#         \"--no-cache-dir\"      # Don't cache to save space\n",
    "#     ])\n",
    "#     print(\"iNLTK installation completed!\")\n",
    "\n",
    "# # Download Malayalam language model for iNLTK\n",
    "# try:\n",
    "#     from inltk.inltk import setup\n",
    "#     print(\"Downloading Malayalam language model...\")\n",
    "#     setup('ml')  # 'ml' is the language code for Malayalam\n",
    "#     print(\"Malayalam model downloaded successfully!\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error downloading Malayalam model: {e}\")\n",
    "#     print(\"You may need to run this cell again or check your internet connection.\")\n",
    "\n",
    "# print(\"iNLTK setup complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a186968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Transformers already available\n"
     ]
    }
   ],
   "source": [
    "# import subprocess\n",
    "# import sys\n",
    "\n",
    "# # Install transformers if not available\n",
    "# try:\n",
    "#     from transformers import AutoTokenizer\n",
    "#     print(\"✓ Transformers already available\")\n",
    "# except ImportError:\n",
    "#     print(\"Installing transformers...\")\n",
    "#     subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"transformers\"], timeout=1800)\n",
    "#     from transformers import AutoTokenizer\n",
    "#     print(\"✓ Transformers installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f88f137d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up modern Malayalam tokenization...\n",
      "=== Setting up Malayalam Tokenization ===\n",
      "\n",
      "--- Trying HuggingFace Transformers ---\n",
      "✓ Transformers already available\n",
      "Trying google/muril-base-cased...\n",
      "✓ google/muril-base-cased works! Sample tokens: ['ഇന്നലെ', 'എനിക്ക്', 'റിപ്പോർട്ട്', 'പൂർത്തിയാക്കാൻ', 'കഴിഞ്ഞില്ല']...\n",
      "✓ Success with HuggingFace google/muril-base-cased\n",
      "\n",
      "=== Testing HuggingFace google/muril-base-cased ===\n",
      "\n",
      "Test 1:\n",
      "Text: ഇന്നലെ എനിക്ക് റിപ്പോർട്ട് പൂർത്തിയാക്കാൻ കഴിഞ്ഞില്ല\n",
      "Tokens: ['ഇന്നലെ', 'എനിക്ക്', 'റിപ്പോർട്ട്', 'പൂർത്തിയാക്കാൻ', 'കഴിഞ്ഞില്ല']\n",
      "Count: 5\n",
      "\n",
      "Test 2:\n",
      "Text: നമസ്കാരം! എങ്ങനെയുണ്ട്?\n",
      "Tokens: ['ന', '##മസ്കാരം', '!', 'എങ്ങനെ', '##യുണ്ട്', '?']\n",
      "Count: 6\n",
      "\n",
      "Test 3:\n",
      "Text: കേരളം എന്നറിയപ്പെടുന്നത് God's Own Country എന്നാണ്.\n",
      "Tokens: ['കേരളം', 'എന്നറിയപ്പെടുന്ന', '##ത്', 'God', \"'\", 's', 'Own', 'Country', 'എന്നാണ്', '.']\n",
      "Count: 10\n",
      "\n",
      "✅ Malayalam tokenization ready using HuggingFace google/muril-base-cased!\n",
      "Use tokenize_malayalam(text) function for your tokenization needs.\n"
     ]
    }
   ],
   "source": [
    "# # Modern Malayalam Tokenizers - Python 3.12 Compatible\n",
    "\n",
    "# import sys\n",
    "# import subprocess\n",
    "\n",
    "# def setup_huggingface_tokenizer():\n",
    "#     \"\"\"Setup HuggingFace tokenizer - BEST option\"\"\"\n",
    "#     try:\n",
    "#         from transformers import AutoTokenizer\n",
    "#         print(\"✓ Transformers already available\")\n",
    "#     except ImportError:\n",
    "#         print(\"Installing transformers...\")\n",
    "#         subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"transformers\"])\n",
    "#         from transformers import AutoTokenizer\n",
    "    \n",
    "#     # Try different multilingual models that support Malayalam\n",
    "#     models_to_try = [\n",
    "#         \"google/muril-base-cased\",  # Multilingual model with Malayalam support\n",
    "#         \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",  # Multilingual\n",
    "#         \"xlm-roberta-base\",  # Covers many languages including Malayalam\n",
    "#     ]\n",
    "    \n",
    "#     for model_name in models_to_try:\n",
    "#         try:\n",
    "#             print(f\"Trying {model_name}...\")\n",
    "#             tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            \n",
    "#             # Test with Malayalam\n",
    "#             malayalam_text = \"ഇന്നലെ എനിക്ക് റിപ്പോർട്ട് പൂർത്തിയാക്കാൻ കഴിഞ്ഞില്ല\"\n",
    "#             tokens = tokenizer.tokenize(malayalam_text)\n",
    "#             print(f\"✓ {model_name} works! Sample tokens: {tokens[:5]}...\")\n",
    "            \n",
    "#             def tokenize_malayalam(text):\n",
    "#                 return tokenizer.tokenize(text)\n",
    "            \n",
    "#             return tokenize_malayalam, f\"HuggingFace {model_name}\"\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"✗ {model_name} failed: {str(e)[:50]}...\")\n",
    "#             continue\n",
    "    \n",
    "#     return None, None\n",
    "\n",
    "# def setup_sentencepiece_tokenizer():\n",
    "#     \"\"\"Setup SentencePiece tokenizer - you already have this installed!\"\"\"\n",
    "#     try:\n",
    "#         import sentencepiece as spm\n",
    "#         print(\"✓ SentencePiece already available\")\n",
    "        \n",
    "#         # Create a simple BPE tokenizer for Malayalam\n",
    "#         # Note: This is a basic setup - for production, you'd train on Malayalam corpus\n",
    "        \n",
    "#         def basic_sentencepiece_tokenize(text):\n",
    "#             # Simple character-level tokenization for Malayalam\n",
    "#             # In production, you'd have a trained Malayalam model\n",
    "#             chars = list(text)\n",
    "#             # Group Malayalam characters, split others\n",
    "#             tokens = []\n",
    "#             current_token = \"\"\n",
    "            \n",
    "#             for char in chars:\n",
    "#                 if '\\u0D00' <= char <= '\\u0D7F':  # Malayalam range\n",
    "#                     current_token += char\n",
    "#                 else:\n",
    "#                     if current_token:\n",
    "#                         tokens.append(current_token)\n",
    "#                         current_token = \"\"\n",
    "#                     if not char.isspace():\n",
    "#                         tokens.append(char)\n",
    "            \n",
    "#             if current_token:\n",
    "#                 tokens.append(current_token)\n",
    "            \n",
    "#             return tokens\n",
    "        \n",
    "#         # Test it\n",
    "#         malayalam_text = \"ഇന്നലെ എനിക്ക് റിപ്പോർട്ട്\"\n",
    "#         tokens = basic_sentencepiece_tokenize(malayalam_text)\n",
    "#         print(f\"✓ Basic SentencePiece working: {tokens}\")\n",
    "        \n",
    "#         return basic_sentencepiece_tokenize, \"SentencePiece (Basic)\"\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"✗ SentencePiece failed: {e}\")\n",
    "#         return None, None\n",
    "\n",
    "# def setup_nltk_tokenizer():\n",
    "#     \"\"\"Setup NLTK with Indic support\"\"\"\n",
    "#     try:\n",
    "#         import nltk\n",
    "#         print(\"✓ NLTK available\")\n",
    "#     except ImportError:\n",
    "#         print(\"Installing NLTK...\")\n",
    "#         subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"nltk\"])\n",
    "#         import nltk\n",
    "    \n",
    "#     try:\n",
    "#         # Download required NLTK data\n",
    "#         nltk.download('punkt', quiet=True)\n",
    "        \n",
    "#         from nltk.tokenize import word_tokenize\n",
    "#         import re\n",
    "        \n",
    "#         def nltk_malayalam_tokenize(text):\n",
    "#             # Use NLTK's word_tokenize and then clean up for Malayalam\n",
    "#             tokens = word_tokenize(text)\n",
    "            \n",
    "#             # Post-process to handle Malayalam better\n",
    "#             clean_tokens = []\n",
    "#             for token in tokens:\n",
    "#                 # If token contains Malayalam, keep as is\n",
    "#                 if re.search(r'[\\u0D00-\\u0D7F]', token):\n",
    "#                     clean_tokens.append(token)\n",
    "#                 # Otherwise, use NLTK's tokenization\n",
    "#                 else:\n",
    "#                     clean_tokens.append(token)\n",
    "            \n",
    "#             return clean_tokens\n",
    "        \n",
    "#         # Test it\n",
    "#         malayalam_text = \"ഇന്നലെ എനിക്ക് റിപ്പോർട്ട് പൂർത്തിയാക്കാൻ കഴിഞ്ഞില്ല\"\n",
    "#         tokens = nltk_malayalam_tokenize(malayalam_text)\n",
    "#         print(f\"✓ NLTK Malayalam working: {tokens[:5]}...\")\n",
    "        \n",
    "#         return nltk_malayalam_tokenize, \"NLTK + Malayalam\"\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"✗ NLTK setup failed: {e}\")\n",
    "#         return None, None\n",
    "\n",
    "# def setup_best_malayalam_tokenizer():\n",
    "#     \"\"\"Try tokenizers in order of preference\"\"\"\n",
    "    \n",
    "#     print(\"=== Setting up Malayalam Tokenization ===\")\n",
    "    \n",
    "#     # Order of preference\n",
    "#     tokenizer_setups = [\n",
    "#         (\"HuggingFace Transformers\", setup_huggingface_tokenizer),\n",
    "#         (\"NLTK\", setup_nltk_tokenizer),\n",
    "#         (\"SentencePiece\", setup_sentencepiece_tokenizer),\n",
    "#     ]\n",
    "    \n",
    "#     for name, setup_func in tokenizer_setups:\n",
    "#         print(f\"\\n--- Trying {name} ---\")\n",
    "#         tokenizer, method = setup_func()\n",
    "        \n",
    "#         if tokenizer:\n",
    "#             print(f\"✓ Success with {method}\")\n",
    "#             return tokenizer, method\n",
    "    \n",
    "#     print(\"\\n✗ All modern tokenizers failed\")\n",
    "#     return None, None\n",
    "\n",
    "# # Main execution\n",
    "# print(\"Setting up modern Malayalam tokenization...\")\n",
    "# tokenize_malayalam, method = setup_best_malayalam_tokenizer()\n",
    "\n",
    "# if tokenize_malayalam:\n",
    "#     # Test with various texts\n",
    "#     test_texts = [\n",
    "#         \"ഇന്നലെ എനിക്ക് റിപ്പോർട്ട് പൂർത്തിയാക്കാൻ കഴിഞ്ഞില്ല\",\n",
    "#         \"നമസ്കാരം! എങ്ങനെയുണ്ട്?\",\n",
    "#         \"കേരളം എന്നറിയപ്പെടുന്നത് God's Own Country എന്നാണ്.\"\n",
    "#     ]\n",
    "    \n",
    "#     print(f\"\\n=== Testing {method} ===\")\n",
    "#     for i, text in enumerate(test_texts, 1):\n",
    "#         tokens = tokenize_malayalam(text)\n",
    "#         print(f\"\\nTest {i}:\")\n",
    "#         print(f\"Text: {text}\")\n",
    "#         print(f\"Tokens: {tokens}\")\n",
    "#         print(f\"Count: {len(tokens)}\")\n",
    "    \n",
    "#     print(f\"\\n✅ Malayalam tokenization ready using {method}!\")\n",
    "#     print(\"Use tokenize_malayalam(text) function for your tokenization needs.\")\n",
    "    \n",
    "# else:\n",
    "#     print(\"\\n❌ No modern tokenizers worked\")\n",
    "#     print(\"Consider using the simple regex-based approach from earlier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0d8b9134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Simple fix for iNLTK import issue\n",
    "# import sys\n",
    "# import subprocess\n",
    "\n",
    "# def setup_inltk():\n",
    "#     try:\n",
    "#         # Try importing first\n",
    "#         from inltk.inltk import setup, tokenize\n",
    "#         setup('ml')\n",
    "#         return True\n",
    "#     except ImportError:\n",
    "#         print(\"Installing iNLTK...\")\n",
    "#         subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"inltk\"], timeout=1800)\n",
    "#         try:\n",
    "#             from inltk.inltk import setup, tokenize\n",
    "#             setup('ml')\n",
    "#             return True\n",
    "#         except:\n",
    "#             return False\n",
    "#     except Exception as e:\n",
    "#         print(f\"iNLTK setup failed: {e}\")\n",
    "#         return False\n",
    "\n",
    "# # Try setup\n",
    "# inltk_ready = setup_inltk()\n",
    "# if inltk_ready:\n",
    "#     print(\"✓ iNLTK ready for Malayalam\")\n",
    "# else:\n",
    "#     print(\"⚠️ Will use fallback tokenization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c15c1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fix for iNLTK sklearn dependency issue\n",
    "# import sys\n",
    "# import subprocess\n",
    "\n",
    "# def install_sklearn_simple():\n",
    "#     \"\"\"Try to install sklearn with a simple approach\"\"\"\n",
    "#     try:\n",
    "#         import sklearn\n",
    "#         print(\"✓ sklearn already available\")\n",
    "#         return True\n",
    "#     except ImportError:\n",
    "#         try:\n",
    "#             print(\"Installing sklearn (this may take a moment)...\")\n",
    "#             # Try a simpler, lighter installation\n",
    "#             subprocess.check_call([\n",
    "#                 sys.executable, \"-m\", \"pip\", \"install\", \n",
    "#                 \"scikit-learn==1.0.2\", \"--no-deps\"\n",
    "#             ], timeout=1800)\n",
    "#             print(\"✓ sklearn installed successfully\")\n",
    "#             return True\n",
    "#         except:\n",
    "#             print(\"❌ Could not install sklearn automatically\")\n",
    "#             return False\n",
    "\n",
    "# # Try to fix the sklearn issue\n",
    "# sklearn_available = install_sklearn_simple()\n",
    "\n",
    "# # Now try iNLTK Malayalam setup again\n",
    "# if sklearn_available:\n",
    "#     try:\n",
    "#         from inltk.inltk import setup, tokenize\n",
    "#         print(\"Attempting Malayalam model setup...\")\n",
    "#         setup('ml')\n",
    "#         print(\"✓ Malayalam model setup successful!\")\n",
    "    \n",
    "#         # Test it\n",
    "#         test_tokens = tokenize(\"ഇന്നലെ എനിക്ക് കഴിഞ്ഞില്ല\", 'ml')\n",
    "#         print(f\"✓ Test successful: {test_tokens}\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"⚠️ iNLTK setup issue: {str(e)}\")\n",
    "#         print(\"Will use fallback tokenization\")\n",
    "# else:\n",
    "#     print(\"⚠️ Using fallback approach due to dependency issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b83fc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Installing missing dependencies for iNLTK...')\n",
    "\n",
    "# # Install scikit-learn (sklearn)\n",
    "# try:\n",
    "#     subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'scikit-learn'], timeout=1800)\n",
    "#     print('✓ scikit-learn installed')\n",
    "# except Exception as e:\n",
    "#     print(f'Error installing scikit-learn: {e}')\n",
    "\n",
    "# # Also install other common dependencies that might be missing\n",
    "# dependencies = ['torch', 'numpy', 'pandas']\n",
    "# for dep in dependencies:\n",
    "#     try:\n",
    "#         __import__(dep)\n",
    "#         print(f'✓ {dep} already available')\n",
    "#     except ImportError:\n",
    "#         try:\n",
    "#             print(f'Installing {dep}...')\n",
    "#             subprocess.check_call([sys.executable, '-m', 'pip', 'install', dep], timeout=120)\n",
    "#             print(f'✓ {dep} installed')\n",
    "#         except Exception as e:\n",
    "#             print(f'Warning: Could not install {dep}: {e}')\n",
    "\n",
    "# print('Dependencies installation complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6ccb57",
   "metadata": {},
   "source": [
    "**ALL NECESSARY PACKAGES INSTALLED ABOVE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57e8b87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2.3 - Tool-based Tokenization using google/muril-base-cased:\n",
      "Using tokenizer: google/muril-base-cased (Multilingual BERT)\n",
      "Number of tokens: 70\n",
      "First 15 tokens: ['ഇന്നലെ', 'എനിക്ക്', 'റിപ്പോർട്ട്', 'പൂർത്തിയാക്കാൻ', 'കഴിഞ്ഞില്ല', '—', 'അത്', 'അത്', '##യാ', '##ധുനിക', 'മോഡൽ', 'ആയിരുന്നു', ',', 'പക്ഷേ', 'ഡാറ്റാ']\n",
      "Last 15 tokens: ['##യേക്കാൾ', 'മികച്ച', '##താണ്', ';', 'എന്നിരുന്നാലും', ',', 'ഞങ്ങൾ', 'ഇപ്പോഴും', 'പരീക്ഷ', '##ിച്ചു', '##ക', '##ൊണ്ട', '##ിരി', '##ക്കുകയാണ്', '.']\n",
      "\n",
      "Key features:\n",
      "- Uses google/muril-base-cased pre-trained model\n",
      "- WordPiece subword tokenization\n",
      "- Handles Malayalam Unicode and complex scripts\n",
      "- Produces linguistically meaningful subword units\n",
      "- Supports mixed Malayalam-English text\n",
      "- Uses ## prefix for subword continuation\n"
     ]
    }
   ],
   "source": [
    "# Q2.3: Tool-based tokenization using google/muril-base-cased\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize the google/muril-base-cased tokenizer\n",
    "muril_tokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")\n",
    "\n",
    "def malayalam_tokenize_text(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tool-based Malayalam tokenizer using google/muril-base-cased:\n",
    "    - Uses pre-trained multilingual BERT model with Malayalam support\n",
    "    - Handles subword tokenization with WordPiece\n",
    "    - Works with mixed Malayalam-English text\n",
    "    - Produces linguistically meaningful subword units\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use the muril tokenizer to tokenize the text\n",
    "    tokens = muril_tokenizer.tokenize(text)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "tool_tokens = malayalam_tokenize_text(paragraph)\n",
    "\n",
    "print(\"Q2.3 - Tool-based Tokenization using google/muril-base-cased:\")\n",
    "print(f\"Using tokenizer: google/muril-base-cased (Multilingual BERT)\")\n",
    "print(f\"Number of tokens: {len(tool_tokens)}\")\n",
    "print(\"First 15 tokens:\", tool_tokens[:15])\n",
    "print(\"Last 15 tokens:\", tool_tokens[-15:])\n",
    "print(\"\\nKey features:\")\n",
    "print(\"- Uses google/muril-base-cased pre-trained model\")\n",
    "print(\"- WordPiece subword tokenization\")\n",
    "print(\"- Handles Malayalam Unicode and complex scripts\")\n",
    "print(\"- Produces linguistically meaningful subword units\")\n",
    "print(\"- Supports mixed Malayalam-English text\")\n",
    "print(\"- Uses ## prefix for subword continuation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f10d1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TOKENIZATION COMPARISON ===\n",
      "\n",
      "Naïve (space): 33 tokens\n",
      "Manual (rules): 42 tokens\n",
      "Tool (google/muril): 70 tokens\n",
      "\n",
      "=== KEY DIFFERENCES ===\n",
      "\n",
      "Tokens in manual but not in tool: 11 unique tokens\n",
      "Tokens in tool but not in manual: 34 unique tokens\n",
      "\n",
      "Sample tokens only in manual: ['അത്യാധുനിക', 'മികച്ചതാണ്', 'ഡാറ്റാസെറ്റ്', 'കെ.എം', 'പരീക്ഷിച്ചുകൊണ്ടിരിക്കുകയാണ്', 'പി.എം.-ന്', 'പഴയവയേക്കാൾ', 'സഹകരിച്ചില്ല', '3:00', 'പരിശോധിക്കും']\n",
      "Sample tokens only in tool: ['മികച്ച', '##ധുനിക', '##ധി', 'പഴയ', '##താണ്', '##ൊണ്ട', '##ിച്ചു', '##ക്കുകയാണ്', '##യേക്കാൾ', '##ക']\n",
      "\n",
      "=== MALAYALAM WORD TOKENIZATION EXAMPLES ===\n",
      "\n",
      "Sample word: 'പരീക്ഷിച്ചുകൊണ്ടിരിക്കുകയാണ്.'\n",
      "  Naïve tokenization:   ['പരീക്ഷിച്ചുകൊണ്ടിരിക്കുകയാണ്.']\n",
      "  Manual tokenization:  ['പരീക്ഷിച്ചുകൊണ്ടിരിക്കുകയാണ്', '.']\n",
      "  Tool tokenization:    ['പരീക്ഷ', '##ിച്ചു', '##ക', '##ൊണ്ട', '##ിരി', '##ക്കുകയാണ്', '.']\n",
      "  Token counts: Naïve=1, Manual=2, Tool=7\n"
     ]
    }
   ],
   "source": [
    "# Q2.4: Comparison of tokenization methods\n",
    "def compare_tokenizations():\n",
    "    \"\"\"Compare the three tokenization methods\"\"\"\n",
    "    \n",
    "    print(\"=== TOKENIZATION COMPARISON ===\\n\")\n",
    "    \n",
    "    methods = [\n",
    "        (\"Naïve (space)\", naive_tokens),\n",
    "        (\"Manual (rules)\", manual_tokens), \n",
    "        (\"Tool (google/muril)\", tool_tokens)\n",
    "    ]\n",
    "    \n",
    "    for name, tokens in methods:\n",
    "        print(f\"{name}: {len(tokens)} tokens\")\n",
    "    \n",
    "    print(\"\\n=== KEY DIFFERENCES ===\")\n",
    "    \n",
    "    # Find differences between manual and tool tokenization\n",
    "    manual_set = set(manual_tokens)\n",
    "    tool_set = set(tool_tokens)\n",
    "    \n",
    "    print(f\"\\nTokens in manual but not in tool: {len(manual_set - tool_set)} unique tokens\")\n",
    "    print(f\"Tokens in tool but not in manual: {len(tool_set - manual_set)} unique tokens\")\n",
    "    \n",
    "    # Show sample differences (first 10 of each)\n",
    "    manual_only = list(manual_set - tool_set)[:10]\n",
    "    tool_only = list(tool_set - manual_set)[:10]\n",
    "    \n",
    "    print(f\"\\nSample tokens only in manual: {manual_only}\")\n",
    "    print(f\"Sample tokens only in tool: {tool_only}\")\n",
    "    \n",
    "    # Analyze specific Malayalam words and their tokenization\n",
    "    malayalam_examples = [\"ഇന്നലെ\", \"പൂർത്തിയാക്കാൻ\", \"അത്യാധുനിക\", \"സഹകരിച്ചില്ല\", \"പരീക്ഷിച്ചുകൊണ്ടിരിക്കുകയാണ്\"]\n",
    "    \n",
    "    # Analyze specific Malayalam words using each tokenization method directly\n",
    "    malayalam_examples = [\"പരീക്ഷിച്ചുകൊണ്ടിരിക്കുകയാണ്.\"]\n",
    "\n",
    "    print(\"\\n=== MALAYALAM WORD TOKENIZATION EXAMPLES ===\")\n",
    "    for example in malayalam_examples:\n",
    "        print(f\"\\nSample word: '{example}'\")\n",
    "        \n",
    "        # Apply each tokenization method directly to the sample word\n",
    "        naive_result = naive_tokenize(example)\n",
    "        manual_result = manual_tokenize(example) \n",
    "        tool_result = malayalam_tokenize_text(example)\n",
    "        \n",
    "        print(f\"  Naïve tokenization:   {naive_result}\")\n",
    "        print(f\"  Manual tokenization:  {manual_result}\")\n",
    "        print(f\"  Tool tokenization:    {tool_result}\")\n",
    "        \n",
    "        # Show token counts\n",
    "        print(f\"  Token counts: Naïve={len(naive_result)}, Manual={len(manual_result)}, Tool={len(tool_result)}\")\n",
    "\n",
    "compare_tokenizations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0f0351d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2.5 - Multiword Expressions (MWEs) Identified v2:\n",
      "Found 5 MWEs in the Malayalam paragraph:\n",
      "\n",
      "1. 'അത്യാധുനിക മോഡൽ'\n",
      "   Reason: Fixed adjectival compound; semantic unity (state-of-the-art model)\n",
      "   Present in text: ✓\n",
      "\n",
      "2. 'ഡോ. കെ.എം. നായരിന്'\n",
      "   Reason: Person's title + name; named entity (Dr. K.M. Nair)\n",
      "   Present in text: ✓\n",
      "\n",
      "3. '3:00 പി.എം.'\n",
      "   Reason: Time expression; numeric + temporal unit (3:00 PM)\n",
      "   Present in text: ✓\n",
      "\n",
      "4. 'പുതിയ മോഡലുകൾ'\n",
      "   Reason: Adjective-noun compound; semantic unity (new models)\n",
      "   Present in text: ✓\n",
      "\n",
      "5. 'പരീക്ഷിച്ചുകൊണ്ടിരിക്കുകയാണ്'\n",
      "   Reason: Complex verbal form; single semantic unit (are testing)\n",
      "   Present in text: ✓\n",
      "\n",
      "WHY TREAT AS SINGLE TOKENS:\n",
      "- MWEs are semantically or syntactically atomic units\n",
      "- Preserves meaning and reduces fragmentation in analysis\n",
      "- Helps downstream NLP tasks (NER, translation, sentiment analysis)\n",
      "- Avoids losing fixed phrasal meanings and cultural concepts\n",
      "- Particularly important for Malayalam compound verbs and complex expressions\n",
      "\n",
      "=== HOW GOOGLE/MURIL HANDLES MWES ===\n",
      "'അത്യാധുനിക മോഡൽ' → ['അത്', '##യാ', '##ധുനിക', 'മോഡൽ']\n",
      "'ഡോ. കെ.എം. നായരിന്' → ['ഡോ', '.', 'കെ', '.', 'എം', '.', 'നായ', '##രിന്']\n",
      "'3:00 പി.എം.' → ['3', ':', '00', 'പി', '.', 'എം', '.']\n",
      "Note: Subword tokenization may split MWEs, but preserves morphological information\n"
     ]
    }
   ],
   "source": [
    "# Q2.5: Multiword Expressions (MWEs) identification\n",
    "def identify_mwes(text: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Identify multiword expressions in the Malayalam text and explain why they should be \n",
    "    treated as single tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    mwes = [\n",
    "        (\"അത്യാധുനിക മോഡൽ\", \"Fixed adjectival compound; semantic unity (state-of-the-art model)\"),\n",
    "        (\"ഡോ. കെ.എം. നായരിന്\", \"Person's title + name; named entity (Dr. K.M. Nair)\"),\n",
    "        (\"3:00 പി.എം.\", \"Time expression; numeric + temporal unit (3:00 PM)\"),\n",
    "        (\"പുതിയ മോഡലുകൾ\", \"Adjective-noun compound; semantic unity (new models)\"),\n",
    "        (\"പരീക്ഷിച്ചുകൊണ്ടിരിക്കുകയാണ്\", \"Complex verbal form; single semantic unit (are testing)\"),\n",
    "    ]\n",
    "    \n",
    "    return mwes\n",
    "\n",
    "mwes = identify_mwes(paragraph)\n",
    "\n",
    "print(\"Q2.5 - Multiword Expressions (MWEs) Identified v2:\")\n",
    "print(f\"Found {len(mwes)} MWEs in the Malayalam paragraph:\\n\")\n",
    "\n",
    "for i, (mwe, reason) in enumerate(mwes, 1):\n",
    "    print(f\"{i}. '{mwe}'\")\n",
    "    print(f\"   Reason: {reason}\")\n",
    "    # Check presence more carefully for Malayalam text\n",
    "    mwe_clean = mwe.replace('.', '').replace(' ', '')\n",
    "    text_clean = paragraph.replace(' ', '')\n",
    "    present = mwe_clean in text_clean or any(word in paragraph for word in mwe.split())\n",
    "    print(f\"   Present in text: {'✓' if present else '✗'}\")\n",
    "    print()\n",
    "\n",
    "print(\"WHY TREAT AS SINGLE TOKENS:\")\n",
    "print(\"- MWEs are semantically or syntactically atomic units\")\n",
    "print(\"- Preserves meaning and reduces fragmentation in analysis\") \n",
    "print(\"- Helps downstream NLP tasks (NER, translation, sentiment analysis)\")\n",
    "print(\"- Avoids losing fixed phrasal meanings and cultural concepts\")\n",
    "print(\"- Particularly important for Malayalam compound verbs and complex expressions\")\n",
    "\n",
    "# Show how google/muril-base-cased handles these MWEs\n",
    "print(f\"\\n=== HOW GOOGLE/MURIL HANDLES MWES ===\")\n",
    "for mwe, reason in mwes[:3]:  # Show first 3 examples\n",
    "    tokens = muril_tokenizer.tokenize(mwe)\n",
    "    print(f\"'{mwe}' → {tokens}\")\n",
    "print(\"Note: Subword tokenization may split MWEs, but preserves morphological information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72446e58",
   "metadata": {},
   "source": [
    "## Q2.6: Reflection on Tokenization\n",
    "\n",
    "**Observation based on the above tokenisation of sentences in Malayalam:**\n",
    "\n",
    "1. **WordPiece vs. Rule-based Tokenization:** The google/muril-base-cased model using WordPiece tokenization handles the Malayalam paragraph much more effectively than naive or manual approaches, breaking complex Malayalam words into meaningful subword units marked with ## continuation symbols that preserve morphological information.\n",
    "\n",
    "2. **Malayalam Script Complexity:** Malayalam's agglutinative nature and complex script with conjuncts, vowel signs, and compound words pose significant challenges for simple tokenization methods, but WordPiece tokenization trained on multilingual data can capture these linguistic patterns more naturally.\n",
    "\n",
    "3. **Subword Granularity Benefits:** The ## prefix system in WordPiece allows the model to represent long Malayalam words (like \"പരീക്ഷിച്ചുകൊണ്ടിരിക്കുകയാണ്\") as sequences of meaningful subword pieces, enabling better handling of unseen words and morphological variations compared to whole-word approaches.\n",
    "\n",
    "4. **Cross-script Robustness:** When processing mixed Malayalam-English text in our paragraph, the pre-trained google/muril model seamlessly handles both scripts without requiring separate tokenization rules, demonstrating the advantage of multilingual models over language-specific manual rules.\n",
    "\n",
    "5. **Trade-offs in Semantic Preservation:** While WordPiece tokenization excels at morphological decomposition and OOV handling, it may fragment semantically coherent units like proper names (\"ഡോ. കെ.എം. നായരിന്\") or compound expressions, requiring careful consideration for downstream tasks that depend on preserving semantic boundaries.\n",
    "\n",
    "**Reflection on the Tokenisation based of the solution**:\n",
    "\n",
    "The most challenging aspect of tokenizing Malayalam is its agglutinative morphology, where multiple suffixes and particles attach to a root word, creating long, complex forms that are difficult to segment correctly. Compared to English, which has more isolated word structures and clearer space delimiters, Malayalam tokenization must handle intricate word boundaries that aren't always separated by spaces. Punctuation, complex morphology, and multiword expressions (MWEs) significantly increase this difficulty, as they require a deeper linguistic understanding beyond simple rules to preserve the correct meaning of tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058e3ff2",
   "metadata": {},
   "source": [
    "# Question 3: Byte Pair Encoding (BPE)\n",
    "\n",
    "**Task:** Implement BPE manually and programmatically, then analyze results.\n",
    "\n",
    "This section covers:\n",
    "1. **Manual BPE** on toy corpus (first 3 merges step-by-step)\n",
    "2. **Programmatic BPE learner** on toy corpus (10 merges)\n",
    "3. **BPE training** on the paragraph from Q2 (30 merges)\n",
    "4. **Analysis and reflection** on subword tokenization\n",
    "\n",
    "**Toy corpus:** \n",
    "`low low low low low lowest lowest newer newer newer newer newer newer wider wider wider new new`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb53f444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3 - Toy Corpus Setup:\n",
      "Original corpus: low low low low low lowest lowest newer newer newer newer newer newer wider wider wider new new\n",
      "Word counts: {'low': 5, 'lowest': 2, 'newer': 6, 'wider': 3, 'new': 2}\n",
      "\n",
      "Initial BPE corpus (character-level with end-of-word markers):\n",
      "'l o w _': 5\n",
      "'l o w e s t _': 2\n",
      "'n e w e r _': 6\n",
      "'w i d e r _': 3\n",
      "'n e w _': 2\n"
     ]
    }
   ],
   "source": [
    "# Define the toy corpus for BPE\n",
    "toy_corpus = \"low low low low low lowest lowest newer newer newer newer newer newer wider wider wider new new\"\n",
    "\n",
    "# Convert to word list and add end-of-word markers\n",
    "words = toy_corpus.split()\n",
    "word_counts = Counter(words)\n",
    "\n",
    "print(\"Q3 - Toy Corpus Setup:\")\n",
    "print(f\"Original corpus: {toy_corpus}\")\n",
    "print(f\"Word counts: {dict(word_counts)}\")\n",
    "\n",
    "# Convert to character-level with end-of-word markers\n",
    "def prepare_bpe_corpus(word_counts):\n",
    "    \"\"\"Prepare corpus for BPE by splitting into characters and adding end-of-word markers\"\"\"\n",
    "    bpe_corpus = {}\n",
    "    for word, count in word_counts.items():\n",
    "        # Split into characters and add end-of-word marker\n",
    "        char_word = ' '.join(list(word)) + ' _'\n",
    "        bpe_corpus[char_word] = count\n",
    "    return bpe_corpus\n",
    "\n",
    "bpe_corpus = prepare_bpe_corpus(word_counts)\n",
    "\n",
    "print(f\"\\nInitial BPE corpus (character-level with end-of-word markers):\")\n",
    "for word, count in bpe_corpus.items():\n",
    "    print(f\"'{word}': {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7ad8aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MANUAL BPE - STEP BY STEP ===\n",
      "\n",
      "STEP 1:\n",
      "Most frequent pair: (e, r) with count 9\n",
      "Merge: e + r → er\n",
      "Updated corpus:\n",
      "  'l o w _': 5\n",
      "  'l o w e s t _': 2\n",
      "  'n e w er _': 6\n",
      "  'w i d er _': 3\n",
      "  'n e w _': 2\n",
      "\n",
      "STEP 2:\n",
      "Most frequent pair: (er, _) with count 9\n",
      "Merge: er + _ → er_\n",
      "Updated corpus:\n",
      "  'l o w _': 5\n",
      "  'l o w e s t _': 2\n",
      "  'n e w er_': 6\n",
      "  'w i d er_': 3\n",
      "  'n e w _': 2\n",
      "\n",
      "STEP 3:\n",
      "Most frequent pair: (n, e) with count 8\n",
      "Merge: n + e → ne\n",
      "Updated corpus:\n",
      "  'l o w _': 5\n",
      "  'l o w e s t _': 2\n",
      "  'ne w er_': 6\n",
      "  'w i d er_': 3\n",
      "  'ne w _': 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q3.1: Manual BPE - First 3 merges step by step\n",
    "def manual_bpe_step_by_step():\n",
    "    \"\"\"Perform manual BPE step by step for the first 3 merges\"\"\"\n",
    "    \n",
    "    # Initial state\n",
    "    current_corpus = bpe_corpus.copy()\n",
    "    merge_history = []\n",
    "    \n",
    "    print(\"=== MANUAL BPE - STEP BY STEP ===\\n\")\n",
    "    \n",
    "    for step in range(1, 4):  # First 3 steps\n",
    "        print(f\"STEP {step}:\")\n",
    "        \n",
    "        # Count all adjacent pairs\n",
    "        pair_counts = defaultdict(int)\n",
    "        for word, count in current_corpus.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pair = (symbols[i], symbols[i + 1])\n",
    "                pair_counts[pair] += count\n",
    "        \n",
    "        if not pair_counts:\n",
    "            break\n",
    "            \n",
    "        # Find most frequent pair\n",
    "        most_frequent_pair = max(pair_counts.items(), key=lambda x: x[1])\n",
    "        pair, freq = most_frequent_pair\n",
    "        \n",
    "        print(f\"Most frequent pair: ({pair[0]}, {pair[1]}) with count {freq}\")\n",
    "        \n",
    "        # Merge the pair\n",
    "        new_symbol = pair[0] + pair[1]\n",
    "        merge_history.append((pair[0], pair[1], new_symbol))\n",
    "        \n",
    "        # Update corpus\n",
    "        new_corpus = {}\n",
    "        for word, count in current_corpus.items():\n",
    "            # Replace the pair in this word\n",
    "            symbols = word.split()\n",
    "            new_symbols = []\n",
    "            i = 0\n",
    "            while i < len(symbols):\n",
    "                if i < len(symbols) - 1 and symbols[i] == pair[0] and symbols[i + 1] == pair[1]:\n",
    "                    new_symbols.append(new_symbol)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_symbols.append(symbols[i])\n",
    "                    i += 1\n",
    "            new_word = ' '.join(new_symbols)\n",
    "            new_corpus[new_word] = count\n",
    "        \n",
    "        current_corpus = new_corpus\n",
    "        \n",
    "        print(f\"Merge: {pair[0]} + {pair[1]} → {new_symbol}\")\n",
    "        print(\"Updated corpus:\")\n",
    "        for word, count in current_corpus.items():\n",
    "            print(f\"  '{word}': {count}\")\n",
    "        print()\n",
    "    \n",
    "    return merge_history, current_corpus\n",
    "\n",
    "manual_merges, manual_final_corpus = manual_bpe_step_by_step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12021fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BPE LEARNER - 10 MERGES ===\n",
      "\n",
      "Merge 1: e + r -> er (freq: 9)\n",
      "Merge 2: er + _ -> er_ (freq: 9)\n",
      "Merge 3: n + e -> ne (freq: 8)\n",
      "Merge 4: ne + w -> new (freq: 8)\n",
      "Merge 5: l + o -> lo (freq: 7)\n",
      "Merge 6: lo + w -> low (freq: 7)\n",
      "Merge 7: new + er_ -> newer_ (freq: 6)\n",
      "Merge 8: low + _ -> low_ (freq: 5)\n",
      "Merge 9: w + i -> wi (freq: 3)\n",
      "Merge 10: wi + d -> wid (freq: 3)\n",
      "\n",
      "Final vocabulary after 10 merges:\n",
      "  'low e s t _': 2\n",
      "  'low_': 5\n",
      "  'new _': 2\n",
      "  'newer_': 6\n",
      "  'wid er_': 3\n"
     ]
    }
   ],
   "source": [
    "# Q3.2: Programmatic BPE learner on toy corpus (10 merges)\n",
    "class BPELearner:\n",
    "    \"\"\"Simple BPE learner implementation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.merges = []\n",
    "        self.vocab = set()\n",
    "    \n",
    "    def get_pairs(self, word_freqs):\n",
    "        \"\"\"Get all adjacent symbol pairs and their frequencies\"\"\"\n",
    "        pairs = defaultdict(int)\n",
    "        for word, freq in word_freqs.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "        return pairs\n",
    "    \n",
    "    def merge_vocab(self, pair, word_freqs):\n",
    "        \"\"\"Merge the most frequent pair in vocabulary\"\"\"\n",
    "        new_word_freqs = {}\n",
    "        bigram = ' '.join(pair)\n",
    "        replacement = ''.join(pair)\n",
    "        \n",
    "        for word in word_freqs:\n",
    "            new_word = word.replace(bigram, replacement)\n",
    "            new_word_freqs[new_word] = word_freqs[word]\n",
    "        \n",
    "        return new_word_freqs\n",
    "    \n",
    "    def learn_bpe(self, word_freqs, num_merges):\n",
    "        \"\"\"Learn BPE merges from word frequencies\"\"\"\n",
    "        self.merges = []\n",
    "        current_word_freqs = word_freqs.copy()\n",
    "        \n",
    "        print(f\"=== BPE LEARNER - {num_merges} MERGES ===\\n\")\n",
    "        \n",
    "        for i in range(num_merges):\n",
    "            pairs = self.get_pairs(current_word_freqs)\n",
    "            if not pairs:\n",
    "                break\n",
    "            \n",
    "            # Get most frequent pair\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            freq = pairs[best_pair]\n",
    "            \n",
    "            print(f\"Merge {i+1}: {best_pair[0]} + {best_pair[1]} -> {''.join(best_pair)} (freq: {freq})\")\n",
    "            \n",
    "            # Apply merge\n",
    "            current_word_freqs = self.merge_vocab(best_pair, current_word_freqs)\n",
    "            self.merges.append(best_pair)\n",
    "        \n",
    "        print(f\"\\nFinal vocabulary after {len(self.merges)} merges:\")\n",
    "        for word, freq in sorted(current_word_freqs.items()):\n",
    "            print(f\"  '{word}': {freq}\")\n",
    "        \n",
    "        return current_word_freqs\n",
    "\n",
    "# Run BPE learner on toy corpus\n",
    "bpe_learner = BPELearner()\n",
    "final_toy_corpus = bpe_learner.learn_bpe(bpe_corpus, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df55dd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BPE SEGMENTATION EXAMPLES ===\n",
      "\n",
      "'new' → ['new', '_']\n",
      "'newer' → ['newer_']\n",
      "'lowest' → ['low', 'e', 's', 't', '_']\n",
      "'wider' → ['wid', 'er_']\n",
      "\n",
      "Invented word:\n",
      "'newestest' → ['new', 'e', 's', 't', 'e', 's', 't', '_']\n",
      "\n",
      "Why subwords help:\n",
      "- Represent unseen (OOV) words as compositions of known pieces\n",
      "- 'newestest' not in training but expressible as subword units\n",
      "- Morpheme alignment: 'er_' aligns with comparative/agent suffix\n"
     ]
    }
   ],
   "source": [
    "# Q3.2: Demonstrate BPE segmentation on examples\n",
    "def apply_bpe_segmentation(word, merges):\n",
    "    \"\"\"Apply learned BPE merges to segment a word\"\"\"\n",
    "    # Start with character-level representation\n",
    "    symbols = list(word) + ['_']\n",
    "    \n",
    "    # Apply each merge in order\n",
    "    for merge in merges:\n",
    "        new_symbols = []\n",
    "        i = 0\n",
    "        while i < len(symbols):\n",
    "            if (i < len(symbols) - 1 and \n",
    "                symbols[i] == merge[0] and \n",
    "                symbols[i + 1] == merge[1]):\n",
    "                new_symbols.append(merge[0] + merge[1])\n",
    "                i += 2\n",
    "            else:\n",
    "                new_symbols.append(symbols[i])\n",
    "                i += 1\n",
    "        symbols = new_symbols\n",
    "    \n",
    "    return symbols\n",
    "\n",
    "print(\"=== BPE SEGMENTATION EXAMPLES ===\\n\")\n",
    "\n",
    "# Test on original words\n",
    "test_words = ['new', 'newer', 'lowest', 'wider']\n",
    "for word in test_words:\n",
    "    segments = apply_bpe_segmentation(word, bpe_learner.merges)\n",
    "    print(f\"'{word}' → {segments}\")\n",
    "\n",
    "# Test on invented word\n",
    "print(f\"\\nInvented word:\")\n",
    "invented_segments = apply_bpe_segmentation('newestest', bpe_learner.merges)\n",
    "print(f\"'newestest' → {invented_segments}\")\n",
    "\n",
    "print(f\"\\nWhy subwords help:\")\n",
    "print(\"- Represent unseen (OOV) words as compositions of known pieces\")\n",
    "print(\"- 'newestest' not in training but expressible as subword units\")\n",
    "print(\"- Morpheme alignment: 'er_' aligns with comparative/agent suffix\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79e4111",
   "metadata": {},
   "source": [
    "## Q3.2: Why use Subword Tokenisation\n",
    "\n",
    "Subword tokenization effectively solves the out-of-vocabulary (OOV) problem by breaking down unknown words into a sequence of known, smaller pieces. For instance, even though the invented word \"newestest\" was not in the original corpus, the BPE model could still represent it by segmenting it into subwords like ['new', 'e', 's', 't', 'e', 's', 't', '_']. \n",
    "\n",
    "This approach ensures that no word is ever truly \"unknown,\" only a new combination of existing vocabulary parts. A perfect example of subwords aligning with meaningful linguistic units, or morphemes, is the creation of the token er_. This subword directly corresponds to the English comparative suffix, as seen in the segmentation of \"newer\" and \"wider.\" By learning this common suffix, the model captures a fundamental piece of English grammar, allowing it to better understand and generalize across different words.\n",
    "\n",
    "Take the word, \"faster\", as an example for an unknown word, not in the BPE corpus. The token er_ would segment the word by separating the known stem from the suffix, resulting in a tokenization like ['fast', 'er_']. This process allows the model to correctly interpret \"faster\" as the comparative form of \"fast,\" even if it had never encountered the specific word \"faster\" during its training, perfectly illustrating its power to generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e96e6ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BPE ON PARAGRAPH (30 MERGES) ===\n",
      "\n",
      "Prepared corpus (31 unique words):\n",
      "  'ഇ ന ന ല _': 1\n",
      "  'എ ന ക ക _': 1\n",
      "  'റ പ പ ർ ട ട _': 1\n",
      "  'പ ർ ത ത യ ക ക ൻ _': 1\n",
      "  'ക ഴ ഞ ഞ ല ല _': 1\n",
      "  'അ ത _': 2\n",
      "  'അ ത യ ധ ന ക _': 1\n",
      "  'മ ഡ ൽ _': 1\n",
      "  'ആ യ ര ന ന _': 1\n",
      "  'പ ക ഷ _': 1\n",
      "  ...\n",
      "=== BPE LEARNER - 30 MERGES ===\n",
      "\n",
      "Merge 1: ക + ക -> കക (freq: 6)\n",
      "Merge 2: ന + ന -> നന (freq: 4)\n",
      "Merge 3: ല + _ -> ല_ (freq: 4)\n",
      "Merge 4: ച + ച -> ചച (freq: 4)\n",
      "Merge 5: ത + യ -> തയ (freq: 3)\n",
      "Merge 6: ൾ + _ -> ൾ_ (freq: 3)\n",
      "Merge 7: നന + ല_ -> നനല_ (freq: 2)\n",
      "Merge 8: കക + _ -> കക_ (freq: 2)\n",
      "Merge 9: പ + പ -> പപ (freq: 2)\n",
      "Merge 10: ൻ + _ -> ൻ_ (freq: 2)\n",
      "Merge 11: ല + ല_ -> ലല_ (freq: 2)\n",
      "Merge 12: അ + ത -> അത (freq: 2)\n",
      "Merge 13: അത + _ -> അത_ (freq: 2)\n",
      "Merge 14: മ + ഡ -> മഡ (freq: 2)\n",
      "Merge 15: ൽ + _ -> ൽ_ (freq: 2)\n",
      "Merge 16: യ + ര -> യര (freq: 2)\n",
      "Merge 17: ക + ഷ -> കഷ (freq: 2)\n",
      "Merge 18: റ + റ -> ററ (freq: 2)\n",
      "Merge 19: ന + _ -> ന_ (freq: 2)\n",
      "Merge 20: പ + ര -> പര (freq: 2)\n",
      "Merge 21: ണ + _ -> ണ_ (freq: 2)\n",
      "Merge 22: ഇ + നനല_ -> ഇനനല_ (freq: 1)\n",
      "Merge 23: എ + ന -> എന (freq: 1)\n",
      "Merge 24: എന + കക_ -> എനകക_ (freq: 1)\n",
      "Merge 25: റ + പപ -> റപപ (freq: 1)\n",
      "Merge 26: റപപ + ർ -> റപപർ (freq: 1)\n",
      "Merge 27: റപപർ + ട -> റപപർട (freq: 1)\n",
      "Merge 28: റപപർട + ട -> റപപർടട (freq: 1)\n",
      "Merge 29: റപപർടട + _ -> റപപർടട_ (freq: 1)\n",
      "Merge 30: പ + ർ -> പർ (freq: 1)\n",
      "\n",
      "Final vocabulary after 30 merges:\n",
      "  '3 0 0 _': 1\n",
      "  'അ യ ചച _': 1\n",
      "  'അ വ ർ _': 1\n",
      "  'അത_': 2\n",
      "  'അതയ ധ ന ക _': 1\n",
      "  'ആ യര നന_': 1\n",
      "  'ഇ പപ ഴ _': 1\n",
      "  'ഇ മ യ ൽ_': 1\n",
      "  'ഇനനല_': 1\n",
      "  'എനകക_': 1\n",
      "  'എനന ര നനല_': 1\n",
      "  'ക എ _': 1\n",
      "  'ക ഴ ഞ ഞ ലല_': 1\n",
      "  'ഞ ങ ങ ൾ_': 1\n",
      "  'ഞ ൻ_': 1\n",
      "  'ഡ _': 1\n",
      "  'ഡ ററ സ ററ _': 1\n",
      "  'ന യര ന_': 1\n",
      "  'ന ള _': 1\n",
      "  'പ എ - ന_': 1\n",
      "  'പ കഷ _': 1\n",
      "  'പ തയ _': 1\n",
      "  'പ ഴ യ വ യ കക ൾ_': 1\n",
      "  'പര കഷ ചച ക ണ ട ര കക ക യ ണ_': 1\n",
      "  'പര ശ ധ കക_': 1\n",
      "  'പർ ത തയ കക ൻ_': 1\n",
      "  'മ ക ചച ത ണ_': 1\n",
      "  'മഡ ല ക ൾ_': 1\n",
      "  'മഡ ൽ_': 1\n",
      "  'റപപർടട_': 1\n",
      "  'സ ഹ ക ര ചച ലല_': 1\n"
     ]
    }
   ],
   "source": [
    "# Q3.3: Train BPE on the paragraph from Q2 (30 merges)\n",
    "def prepare_paragraph_for_bpe(text: str):\n",
    "    \"\"\"Prepare paragraph for BPE training\"\"\"\n",
    "    # Lowercase and tokenize\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_freqs = Counter(words)\n",
    "    \n",
    "    # Convert to character-level with end-of-word markers\n",
    "    bpe_word_freqs = {}\n",
    "    for word, freq in word_freqs.items():\n",
    "        # Remove punctuation and split into characters\n",
    "        clean_word = ''.join(c for c in word if c.isalnum() or c in \"'-\")\n",
    "        if clean_word:\n",
    "            char_word = ' '.join(list(clean_word)) + ' _'\n",
    "            bpe_word_freqs[char_word] = freq\n",
    "    \n",
    "    return bpe_word_freqs\n",
    "\n",
    "# Prepare paragraph for BPE\n",
    "paragraph_bpe_corpus = prepare_paragraph_for_bpe(paragraph)\n",
    "\n",
    "print(\"=== BPE ON PARAGRAPH (30 MERGES) ===\\n\")\n",
    "print(f\"Prepared corpus ({len(paragraph_bpe_corpus)} unique words):\")\n",
    "for word, freq in list(paragraph_bpe_corpus.items())[:10]:  # Show first 10\n",
    "    print(f\"  '{word}': {freq}\")\n",
    "print(\"  ...\")\n",
    "\n",
    "# Learn BPE on paragraph\n",
    "paragraph_bpe_learner = BPELearner()\n",
    "final_paragraph_corpus = paragraph_bpe_learner.learn_bpe(paragraph_bpe_corpus, 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "510c0c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PARAGRAPH BPE ANALYSIS ===\n",
      "\n",
      "Top 5 merges (first 5 applied):\n",
      "  1. ക + ക -> കക\n",
      "  2. ന + ന -> നന\n",
      "  3. ല + _ -> ല_\n",
      "  4. ച + ച -> ചച\n",
      "  5. ത + യ -> തയ\n",
      "\n",
      "Five longest subword tokens learned:\n",
      "  ['റപപർടട_', 'ഇനനല_', 'എനകക_', 'നനല_', 'അതയ']\n",
      "\n",
      "BPE segmentations for top 5 longest Malayalam words:\n",
      "  1. 'പരകഷചചകണടരകകകയണ' (length: 15) -> ['പര', 'കഷ', 'ചച', 'ക', 'ണ', 'ട', 'ര', 'കക', 'ക', 'യ', 'ണ_']\n",
      "  2. 'പഴയവയകകൾ' (length: 8) -> ['പ', 'ഴ', 'യ', 'വ', 'യ', 'കക', 'ൾ_']\n",
      "  3. 'സഹകരചചലല' (length: 8) -> ['സ', 'ഹ', 'ക', 'ര', 'ചച', 'ലല_']\n",
      "  4. 'പർതതയകകൻ' (length: 8) -> ['പർ', 'ത', 'തയ', 'കക', 'ൻ_']\n",
      "  5. 'എനനരനനല' (length: 7) -> ['എ', 'നന', 'ര', 'നനല_']\n",
      "\n",
      "Note: These are the longest words in the Malayalam paragraph, showing how BPE handles complex Malayalam morphology.\n"
     ]
    }
   ],
   "source": [
    "# Q3.3: Analysis of paragraph BPE results\n",
    "print(\"=== PARAGRAPH BPE ANALYSIS ===\\n\")\n",
    "\n",
    "# Show top 5 merges\n",
    "print(\"Top 5 merges (first 5 applied):\")\n",
    "for i, merge in enumerate(paragraph_bpe_learner.merges[:5], 1):\n",
    "    print(f\"  {i}. {merge[0]} + {merge[1]} -> {''.join(merge)}\")\n",
    "\n",
    "# Find longest subword tokens\n",
    "all_subwords = set()\n",
    "for word in final_paragraph_corpus.keys():\n",
    "    subwords = word.split()\n",
    "    all_subwords.update(subwords)\n",
    "\n",
    "# Sort by length and show longest\n",
    "longest_subwords = sorted(all_subwords, key=len, reverse=True)[:5]\n",
    "print(f\"\\nFive longest subword tokens learned:\")\n",
    "print(f\"  {longest_subwords}\")\n",
    "\n",
    "# Get top 5 longest words from Malayalam paragraph\n",
    "malayalam_words = paragraph.split()\n",
    "# Clean words (remove punctuation) and keep unique words\n",
    "unique_clean_words = set()\n",
    "for word in malayalam_words:\n",
    "    clean_word = ''.join(c for c in word if c.isalnum())\n",
    "    if clean_word:\n",
    "        unique_clean_words.add(clean_word.lower())\n",
    "\n",
    "# Sort by length and get top 5 longest\n",
    "top_5_longest = sorted(unique_clean_words, key=len, reverse=True)[:5]\n",
    "\n",
    "# Demonstrate segmentation on top 5 longest Malayalam words\n",
    "print(f\"\\nBPE segmentations for top 5 longest Malayalam words:\")\n",
    "for i, word in enumerate(top_5_longest, 1):\n",
    "    segments = apply_bpe_segmentation(word, paragraph_bpe_learner.merges)\n",
    "    print(f\"  {i}. '{word}' (length: {len(word)}) -> {segments}\")\n",
    "\n",
    "print(f\"\\nNote: These are the longest words in the Malayalam paragraph, showing how BPE handles complex Malayalam morphology.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006e4f6d",
   "metadata": {},
   "source": [
    "## Q3.4: Reflection on BPE\n",
    "\n",
    "**Observation and Reflections from the mini-BPE that we just learned**\n",
    "\n",
    "The BPE algorithm learned a variety of subword types, reflecting a hierarchy from simple character combinations to more linguistically meaningful units.\n",
    "\n",
    "- Meaningful Suffixes: The most interesting learned tokens were common Malayalam suffixes, which are true morphemes. For example, ൾ_ (Merge 6) is a standard plural marker, and ൽ_ (Merge 15) often represents the locative case (meaning \"in\" or \"at\").\n",
    "\n",
    "- Whole Words: For very high-frequency words, the algorithm managed to reconstruct the entire word. A great example is അത്_ (ath_, meaning \"it\" or \"that\"), which was formed in Merge 13, and ഇന്നലെ_ (innaley_, meaning \"yesterday\") in Merge 22.\n",
    "\n",
    "- Common Character Combinations: Many of the initial merges were simply statistically frequent character pairs or consonant clusters, not necessarily meaningful units on their own. Examples include കക (kk), നന (nn), and തയ (thay). These act as basic building blocks for larger tokens.\n",
    "\n",
    "**NOTE**: *With only 30 merges, the process primarily captured suffixes and very frequent short words. It didn't progress enough to consistently isolate word stems or prefixes.*\n",
    "\n",
    "\n",
    "**Pros and Cons**\n",
    "\n",
    "Subword tokenization is a powerful technique for morphologically rich languages like Malayalam, but it comes with trade-offs.\n",
    "\n",
    " **Pros** \n",
    "\n",
    "   1. Handles Agglutinative Morphology: \n",
    "    \n",
    "      Malayalam words are often formed by adding multiple suffixes to a root stem. Subword tokenization naturally handles this by breaking words into a stem and its constituent suffixes. This allows a model to recognize the relationship between പോകുന്നു (pokunnu - \"is going\") and പോയി (poyi - \"went\") by seeing a common stem token, drastically reducing the vocabulary size and helping it generalize to unseen word forms.\n",
    "\n",
    "   2. Manages Compound Words and Loanwords: \n",
    "    \n",
    "      The method gracefully handles both native compound words (സമാസപദം) and the many English loanwords in Malayalam like \"റിപ്പോർട്ട്\" (report). Instead of treating \"റിപ്പോർട്ട്\" as an unknown out-of-vocabulary (OOV) word, BPE breaks it down into manageable subwords (like റപപർടട_ in your output), allowing the model to process it effectively.\n",
    "\n",
    " **Cons**\n",
    "\n",
    "   1. Creates Non-Meaningful Splits: \n",
    "    \n",
    "      Since BPE is driven purely by frequency, it can create subwords that have no linguistic or semantic meaning. For example, splitting a word based on a common but meaningless character pair can make it harder for the model to learn the true compositional meaning of the word. A split might be statistically optimal but linguistically nonsensical.\n",
    "\n",
    "   2. Struggles with \"Sandhi\": \n",
    "    \n",
    "      Malayalam uses complex \"Sandhi\" rules, where sounds at word junctions merge and change (e.g., അതി + ഇൽ → അതിൽ). A BPE model trained on a small corpus might not learn these phonetic rules and could split a merged word in an unnatural way that obscures the original root words, making it difficult for a model to capture the underlying semantics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dcabea",
   "metadata": {},
   "source": [
    "# Question 4: Edit Distance\n",
    "\n",
    "**Task:** Compute edit distance between \"Sunday\" and \"Saturday\" using dynamic programming with two different cost models.\n",
    "\n",
    "This section covers:\n",
    "1. **Model A:** Substitution = 1, Insertion = 1, Deletion = 1\n",
    "2. **Model B:** Substitution = 2, Insertion = 1, Deletion = 1\n",
    "3. **Dynamic programming implementation** with full DP table\n",
    "4. **Backtracing** to find optimal edit sequences\n",
    "5. **Analysis and reflection** on cost model effects\n",
    "\n",
    "**String pair:** `sunday` → `saturday`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "651a65e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q4 - Edit Distance: 'sunday' → 'saturday'\n",
      "Source length: 6, Target length: 8\n"
     ]
    }
   ],
   "source": [
    "# Q4: Edit Distance Implementation\n",
    "class EditDistance:\n",
    "    \"\"\"Edit distance calculator with dynamic programming and backtracing\"\"\"\n",
    "    \n",
    "    def __init__(self, sub_cost=1, ins_cost=1, del_cost=1):\n",
    "        self.sub_cost = sub_cost\n",
    "        self.ins_cost = ins_cost\n",
    "        self.del_cost = del_cost\n",
    "    \n",
    "    def compute_distance(self, src: str, tgt: str):\n",
    "        \"\"\"Compute edit distance using dynamic programming\"\"\"\n",
    "        m, n = len(src), len(tgt)\n",
    "        \n",
    "        # Initialize DP table\n",
    "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "        \n",
    "        # Base cases\n",
    "        for i in range(m + 1):\n",
    "            dp[i][0] = i * self.del_cost\n",
    "        for j in range(n + 1):\n",
    "            dp[0][j] = j * self.ins_cost\n",
    "        \n",
    "        # Fill DP table\n",
    "        for i in range(1, m + 1):\n",
    "            for j in range(1, n + 1):\n",
    "                if src[i-1] == tgt[j-1]:\n",
    "                    # Match - no cost\n",
    "                    dp[i][j] = dp[i-1][j-1]\n",
    "                else:\n",
    "                    # Take minimum of three operations\n",
    "                    substitute = dp[i-1][j-1] + self.sub_cost\n",
    "                    insert = dp[i][j-1] + self.ins_cost\n",
    "                    delete = dp[i-1][j] + self.del_cost\n",
    "                    dp[i][j] = min(substitute, insert, delete)\n",
    "        \n",
    "        return dp, dp[m][n]\n",
    "    \n",
    "    def backtrace(self, dp, src: str, tgt: str):\n",
    "        \"\"\"Backtrace to find optimal edit sequence\"\"\"\n",
    "        i, j = len(src), len(tgt)\n",
    "        operations = []\n",
    "        \n",
    "        while i > 0 or j > 0:\n",
    "            if i > 0 and j > 0 and src[i-1] == tgt[j-1]:\n",
    "                # Match\n",
    "                operations.append(f\"match {src[i-1]} → {tgt[j-1]}\")\n",
    "                i -= 1\n",
    "                j -= 1\n",
    "            elif (i > 0 and j > 0 and \n",
    "                  dp[i][j] == dp[i-1][j-1] + self.sub_cost):\n",
    "                # Substitution\n",
    "                operations.append(f\"substitute {src[i-1]} → {tgt[j-1]}\")\n",
    "                i -= 1\n",
    "                j -= 1\n",
    "            elif j > 0 and dp[i][j] == dp[i][j-1] + self.ins_cost:\n",
    "                # Insertion\n",
    "                operations.append(f\"insert {tgt[j-1]}\")\n",
    "                j -= 1\n",
    "            elif i > 0 and dp[i][j] == dp[i-1][j] + self.del_cost:\n",
    "                # Deletion\n",
    "                operations.append(f\"delete {src[i-1]}\")\n",
    "                i -= 1\n",
    "        \n",
    "        return list(reversed(operations))\n",
    "\n",
    "# Define the strings\n",
    "src_string = \"sunday\"\n",
    "tgt_string = \"saturday\"\n",
    "\n",
    "print(f\"Q4 - Edit Distance: '{src_string}' → '{tgt_string}'\")\n",
    "print(f\"Source length: {len(src_string)}, Target length: {len(tgt_string)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0883004d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL A: Sub=1, Ins=1, Del=1 ===\n",
      "\n",
      "Minimum edit distance: 3\n",
      "\n",
      "Model A DP Table\n",
      "DP Table:\n",
      "    ε   s   a   t   u   r   d   a   y\n",
      " ε:  0   1   2   3   4   5   6   7   8\n",
      " s:  1   0   1   2   3   4   5   6   7\n",
      " u:  2   1   1   2   2   3   4   5   6\n",
      " n:  3   2   2   2   3   3   4   5   6\n",
      " d:  4   3   3   3   3   4   3   4   5\n",
      " a:  5   4   3   4   4   4   4   3   4\n",
      " y:  6   5   4   4   5   5   5   4   3\n",
      "\n",
      "Optimal edit sequence (one possible path):\n",
      "  1. match s → s\n",
      "  2. insert a\n",
      "  3. insert t\n",
      "  4. match u → u\n",
      "  5. substitute n → r\n",
      "  6. match d → d\n",
      "  7. match a → a\n",
      "  8. match y → y\n",
      "\n",
      "Operation breakdown:\n",
      "  Match: 5\n",
      "  Substitute: 1\n",
      "  Insert: 2\n",
      "\n",
      "Cost breakdown: 2×1 + 1×1 = 3\n"
     ]
    }
   ],
   "source": [
    "# Q4.1: Model A (Sub=1, Ins=1, Del=1)\n",
    "print(\"=== MODEL A: Sub=1, Ins=1, Del=1 ===\\n\")\n",
    "\n",
    "model_a = EditDistance(sub_cost=1, ins_cost=1, del_cost=1)\n",
    "dp_a, distance_a = model_a.compute_distance(src_string, tgt_string)\n",
    "\n",
    "print(f\"Minimum edit distance: {distance_a}\")\n",
    "\n",
    "# Display DP table\n",
    "def display_dp_table(dp, src, tgt, title):\n",
    "    \"\"\"Display the DP table in a readable format\"\"\"\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"DP Table:\")\n",
    "    \n",
    "    # Header\n",
    "    header = \"    ε  \" + \"  \".join(f\"{c:>2}\" for c in tgt)\n",
    "    print(header)\n",
    "    \n",
    "    # Rows\n",
    "    for i, row in enumerate(dp):\n",
    "        if i == 0:\n",
    "            row_label = \"ε\"\n",
    "        else:\n",
    "            row_label = src[i-1]\n",
    "        row_str = f\"{row_label:>2}: \" + \"  \".join(f\"{val:>2}\" for val in row)\n",
    "        print(row_str)\n",
    "\n",
    "display_dp_table(dp_a, src_string, tgt_string, \"Model A DP Table\")\n",
    "\n",
    "# Backtrace\n",
    "operations_a = model_a.backtrace(dp_a, src_string, tgt_string)\n",
    "print(f\"\\nOptimal edit sequence (one possible path):\")\n",
    "for i, op in enumerate(operations_a, 1):\n",
    "    print(f\"  {i}. {op}\")\n",
    "\n",
    "# Count operation types\n",
    "op_counts_a = {\"match\": 0, \"substitute\": 0, \"insert\": 0, \"delete\": 0}\n",
    "for op in operations_a:\n",
    "    for op_type in op_counts_a:\n",
    "        if op.startswith(op_type):\n",
    "            op_counts_a[op_type] += 1\n",
    "\n",
    "print(f\"\\nOperation breakdown:\")\n",
    "for op_type, count in op_counts_a.items():\n",
    "    if count > 0:\n",
    "        print(f\"  {op_type.capitalize()}: {count}\")\n",
    "\n",
    "print(f\"\\nCost breakdown: {op_counts_a['insert']}×{model_a.ins_cost} + {op_counts_a['substitute']}×{model_a.sub_cost} = {distance_a}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85ed8bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL B: Sub=2, Ins=1, Del=1 ===\n",
      "\n",
      "Minimum edit distance: 4\n",
      "\n",
      "Model B DP Table\n",
      "DP Table:\n",
      "    ε   s   a   t   u   r   d   a   y\n",
      " ε:  0   1   2   3   4   5   6   7   8\n",
      " s:  1   0   1   2   3   4   5   6   7\n",
      " u:  2   1   2   3   2   3   4   5   6\n",
      " n:  3   2   3   4   3   4   5   6   7\n",
      " d:  4   3   4   5   4   5   4   5   6\n",
      " a:  5   4   3   4   5   6   5   4   5\n",
      " y:  6   5   4   5   6   7   6   5   4\n",
      "\n",
      "Optimal edit sequence (one possible path):\n",
      "  1. match s → s\n",
      "  2. insert a\n",
      "  3. insert t\n",
      "  4. match u → u\n",
      "  5. substitute n → r\n",
      "  6. match d → d\n",
      "  7. match a → a\n",
      "  8. match y → y\n",
      "\n",
      "Operation breakdown:\n",
      "  Match: 5\n",
      "  Substitute: 1\n",
      "  Insert: 2\n",
      "\n",
      "Cost breakdown: 2×1 + 1×2 + 0×1 = 4\n"
     ]
    }
   ],
   "source": [
    "# Q4.2: Model B (Sub=2, Ins=1, Del=1)\n",
    "print(\"=== MODEL B: Sub=2, Ins=1, Del=1 ===\\n\")\n",
    "\n",
    "model_b = EditDistance(sub_cost=2, ins_cost=1, del_cost=1)\n",
    "dp_b, distance_b = model_b.compute_distance(src_string, tgt_string)\n",
    "\n",
    "print(f\"Minimum edit distance: {distance_b}\")\n",
    "\n",
    "display_dp_table(dp_b, src_string, tgt_string, \"Model B DP Table\")\n",
    "\n",
    "# Backtrace\n",
    "operations_b = model_b.backtrace(dp_b, src_string, tgt_string)\n",
    "print(f\"\\nOptimal edit sequence (one possible path):\")\n",
    "for i, op in enumerate(operations_b, 1):\n",
    "    print(f\"  {i}. {op}\")\n",
    "\n",
    "# Count operation types\n",
    "op_counts_b = {\"match\": 0, \"substitute\": 0, \"insert\": 0, \"delete\": 0}\n",
    "for op in operations_b:\n",
    "    for op_type in op_counts_b:\n",
    "        if op.startswith(op_type):\n",
    "            op_counts_b[op_type] += 1\n",
    "\n",
    "print(f\"\\nOperation breakdown:\")\n",
    "for op_type, count in op_counts_b.items():\n",
    "    if count > 0:\n",
    "        print(f\"  {op_type.capitalize()}: {count}\")\n",
    "\n",
    "total_cost = (op_counts_b['insert'] * model_b.ins_cost + \n",
    "              op_counts_b['substitute'] * model_b.sub_cost + \n",
    "              op_counts_b['delete'] * model_b.del_cost)\n",
    "print(f\"\\nCost breakdown: {op_counts_b['insert']}×{model_b.ins_cost} + {op_counts_b['substitute']}×{model_b.sub_cost} + {op_counts_b['delete']}×{model_b.del_cost} = {total_cost}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cfe54b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPARISON OF MODELS A & B ===\n",
      "\n",
      "Model A (Sub=1, Ins=1, Del=1): Distance = 3\n",
      "Model B (Sub=2, Ins=1, Del=1): Distance = 4\n",
      "Difference: 1\n",
      "\n",
      "Alignment comparison:\n",
      "Model A operations: 8 total\n",
      "Model B operations: 8 total\n",
      "\n",
      "Model A sequence:\n",
      "  match s → s\n",
      "  insert a\n",
      "  insert t\n",
      "  match u → u\n",
      "  substitute n → r\n",
      "  match d → d\n",
      "  match a → a\n",
      "  match y → y\n",
      "\n",
      "Model B sequence:\n",
      "  match s → s\n",
      "  insert a\n",
      "  insert t\n",
      "  match u → u\n",
      "  substitute n → r\n",
      "  match d → d\n",
      "  match a → a\n",
      "  match y → y\n",
      "\n",
      "Same edit sequence? True\n",
      "The algorithms found the same optimal alignment.\n",
      "The difference in distance is due to different substitution costs.\n"
     ]
    }
   ],
   "source": [
    "# Q4.3: Comparison of the two models\n",
    "print(\"=== COMPARISON OF MODELS A & B ===\\n\")\n",
    "\n",
    "print(f\"Model A (Sub=1, Ins=1, Del=1): Distance = {distance_a}\")\n",
    "print(f\"Model B (Sub=2, Ins=1, Del=1): Distance = {distance_b}\")\n",
    "print(f\"Difference: {distance_b - distance_a}\")\n",
    "\n",
    "print(f\"\\nAlignment comparison:\")\n",
    "print(f\"Model A operations: {len(operations_a)} total\")\n",
    "print(f\"Model B operations: {len(operations_b)} total\")\n",
    "\n",
    "print(f\"\\nModel A sequence:\")\n",
    "for op in operations_a:\n",
    "    print(f\"  {op}\")\n",
    "\n",
    "print(f\"\\nModel B sequence:\")\n",
    "for op in operations_b:\n",
    "    print(f\"  {op}\")\n",
    "\n",
    "# Check if sequences are the same\n",
    "same_sequence = operations_a == operations_b\n",
    "print(f\"\\nSame edit sequence? {same_sequence}\")\n",
    "\n",
    "if same_sequence:\n",
    "    print(\"The algorithms found the same optimal alignment.\")\n",
    "    print(\"The difference in distance is due to different substitution costs.\")\n",
    "else:\n",
    "    print(\"The algorithms found different optimal alignments.\")\n",
    "    print(\"This shows how cost models can affect the preferred edit path.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241e3644",
   "metadata": {},
   "source": [
    "\n",
    "## Q4.4: Reflection on Edit Distance \n",
    "\n",
    "**Reflection on Edit Distance Model Comparison**\n",
    "\n",
    "* The two models produced different distances \n",
    "    - Model B assigns a higher cost of 2 for a substitution, while Model A's cost is only 1.\n",
    "* For transforming \"Sunday\" to \"Saturday,\" the most useful operations were two insertions and one substitution.\n",
    "* The choice of model is critical for different applications:\n",
    "    * **Spell Check:** For spell-checking tasks where single-character substitutions are common typos, a low substitution cost makes sense. Model A's equal costs are generally better, as common typos like substitutions, insertions, or deletions are treated as equally probable errors.\n",
    "    * **DNA Alignment:**  In genetics, a substitution (a point mutation) is a fundamentally different biological event than an insertion or deletion (an indel). In DNA sequence alignment, costs should reflect real biological mutation rates. Model B's variable costs are superior, as they can represent the different biological probabilities of a mutation (substitution) versus an indel (insertion/deletion)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6935e4c4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
